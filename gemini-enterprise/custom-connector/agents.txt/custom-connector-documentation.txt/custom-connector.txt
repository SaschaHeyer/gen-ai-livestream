Sidebar menu
Search
Write
Notifications1

Sascha Heyer
Home
Library
Profile
Stories
Stats
Following
The Startup
The Startup
Towards AI
Towards AI
Artificial Intelligence in Plain English
Artificial Intelligence in Plain English
AWS in Plain English
AWS in Plain English
Muzli
Muzli
Entrepreneurship Handbook
Entrepreneurship Handbook
Google Cloud - Community
Google Cloud - Community
Data Science Collective
Data Science Collective
Mayurkumar Surani
Mayurkumar Surani
UX Collective
UX Collective

More
Google Cloud - Community
Google Cloud - Community
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.

Following

Integrating knowledge systems with Google Agentspace — custom Connectors/Datastores
Olejniczak Lukasz
Olejniczak Lukasz

Following
8 min read
·
Sep 15, 2025
17


1





This article is co-authored by Łukasz Olejniczak and Rafał Knapik @raknapik (rafal.knapik@allegro.com).

Agentspace is a new Google Cloud service that allows company employees and agents to securely search and act on private company data, while respecting all data privacy and access controls. It provides a single, unified place for employees to ask questions and get answers by searching across connected knowledge sources such as SharePoint, OneDrive, ServiceNow, Confluence, Google Drive and many more.

Press enter or click to view image in full size

Connecting sources of knowledge requires the use of connectors, which fall into two main types:

Indexing Connectors: These connectors pull both data and access control information from a source system. They then represent this data as embeddings and metadata, persisting it in Agentspace datastores where it can be used by Agentspace’s knowledge assistants.
Transactional Connectors: These connectors push knowledge discovery requests directly to the source systems.
In this blog post, we will focus on indexing connectors. While Agentspace has a growing list of built-in connectors and chances are you will find on this list all main sources of knowledge used in your company, it’s not feasible for Google to build connector for every possible system.

Therefore, this article will demonstrate how to create a custom connector or custom datastore to bring data and access control information from unsupported systems, making them available to Agentspace Knowledge Assistants.

Press enter or click to view image in full size

Some of you may ask why we can’t just use a MCP Servers, which seems similar to transactional connector. The answer is simple: pushing a search directly to the source system means relying on the quality of its search module. Many legacy systems use simple keyword-based methods, and the quality of this component will directly impact the answers generated by Agentspace assistants.

Agentspace, on the other hand, uses sophisticated search and ranking algorithms, along with data augmentation techniques that utilize knowledge graphs. These are all tuned for internal knowledge representations and will likely perform much better than the native search mechanisms in many legacy systems.

The Agentspace knowledge activation layer, which includes impressive components like NotebookLM and Agent Garden with first-party and custom agents, is also an important part of the platform. However, that layer is not in the scope of this post. Here, we’ll lay the groundwork for future blog posts by demonstrating how to add custom datastores to Agentspace knowledge assistants.

The Custom Connector Pipeline
A full data and access control synchronization pipeline has at least three stages:

Connect to the knowledge source system to pull both data and access control information.
Blend the data and access control info into a JSON document that aligns with the Datastore document schema.
Import these documents into the Datastore.
Press enter or click to view image in full size

For this guide, we’ll simplify the process and assume you already have a list of documents ready to import into a Datastore:

Press enter or click to view image in full size

Now, let’s get our hands dirty. First, we need a Datastore, which works as a container for our indexed data, metadata, and access control information.

def get_or_create_data_store(
    project_id: str,
    location: str,
    data_store_id: str,
) -> discoveryengine.DataStore:
    """Get or create a DataStore."""

    client_options = (
        ClientOptions(api_endpoint=f"{location}-discoveryengine.googleapis.com")
        if location != "global"
        else None
    )
    client = discoveryengine.DataStoreServiceClient(client_options=client_options)
    ds_name = client.data_store_path(project_id, location, data_store_id)
    try:
        return client.get_data_store(request={"name": ds_name})
    except NotFound:
        parent = client.collection_path(project_id, location, "default_collection")
        operation = client.create_data_store(
            request={
                "parent": parent,
                "data_store": discoveryengine.DataStore(
                    display_name=data_store_id,
                    industry_vertical=discoveryengine.IndustryVertical.GENERIC,
                    acl_enabled=True, ########## we need authorized datastores
                ),
                "data_store_id": data_store_id,
            }
        )
        return operation.result()
The key property here is acl_enabled=True, which ensures that the Datastore enables access control.

acl_enabled=True,
For this exercise, our knowledge entities will be tickets from an IT system.

Press enter or click to view image in full size

I used Gemini to mock 10 tickets in a JSON file, which serves as our batch of data extracted from the source system.

Press enter or click to view image in full size

Next, we need an auxiliary function to map each task object into a discoveryengine.Document object, which the Datastore expects. This function takes a list of tasks and returns a list of discoveryengine.Document objects.

def convert_posts_to_documents(tasks: List[dict]) -> List[discoveryengine.Document]:
    """Convert tasks into Discovery Engine Document messages."""
    docs: List[discoveryengine.Document] = []
    for task in tasks:
        payload = {
            "title": task.get("task-title", {}),
            "body": {
                "description": task.get("task_description", {}),
                "assigned_to": task.get("employee", {}),
                "assigned_to_email": task.get("employee_email", {}),
                "priority": task.get("priority", {}),
                "status": task.get("status", {}),
                "creation_date": task.get("creation_date", {}),
                "task_id": task.get("task_id")

            },
            "url": "http://localhost",
            "author": task.get("author"),
            "categories": [task.get("category")],
            "tags": task.get("tags"),
            "date": task.get("creation_date"),
        }
        doc = discoveryengine.Document(
            id=str(task.get("task_id")),
            json_data=json.dumps(payload),
            acl_info=discoveryengine.Document.AclInfo(
                readers=[{
                    "principals": [
                        {"user_id": task.get("employee_email", {})}
                    ]
                }]
            ),
        )
        docs.append(doc)
    return docs
The acl_info section is where we define who can access the document. In this simple example, we use the employee_email from the task object to specify a single user who can read the record. Of course, you can assign more principals and user groups as needed.

acl_info=discoveryengine.Document.AclInfo(
                readers=[{
                    "principals": [
                        {"user_id": task.get("employee_email", {})}
                    ]
                }]
            )
Now when we know how to map records from our source system to Datastore documents we can import tickets as documents to our Datastore:

def upload_documents_inline(
    project_id: str,
    location: str,
    data_store_id: str,
    branch_id: str,
    documents: List[discoveryengine.Document],
) -> discoveryengine.ImportDocumentsMetadata:
    """Inline import of Document messages."""

    client_options = (
        ClientOptions(api_endpoint=f"{location}-discoveryengine.googleapis.com")
        if location != "global"
        else None
    )

    client = discoveryengine.DocumentServiceClient(client_options=client_options)
    parent = client.branch_path(
        project=project_id,
        location=location,
        data_store=data_store_id,
        branch=branch_id,
    )
    request = discoveryengine.ImportDocumentsRequest(
        parent=parent,
        inline_source=discoveryengine.ImportDocumentsRequest.InlineSource(
            documents=documents,
        ),
    )
    operation = client.import_documents(request=request)
    operation.result()
    return operation.metadata
Since our set of tasks is very small, we can specify an inline_source for the ImportDocumentsRequest without risking out-of-memory errors. This method uses the in-memory collection of documents directly:

inline_source = discoveryengine.ImportDocumentsRequest.InlineSource(
    documents=documents,
)
In real-world situations, however, ingestion pipelines should first stage the source records (upload them) to Cloud Storage or BigQuery. The import job must then use that staged location as its source rather than using an in-memory collection.

Testing the Custom Datastore
At this point, we have our custom datastore filled with sample data, and each imported document includes information about access control. We can now go to the Agentspace application and ask the assistant to list our tickets, expecting to see only those tickets we are allowed to see.

Now, let’s create a new Agentspace application:

Press enter or click to view image in full size

Give it a name, choose serving location, and provide your company name to help Agentspace Assistant ground responses in the context of your company.

Press enter or click to view image in full size

Press enter or click to view image in full size

Next, we will attach our custom datastore to it by choosing Add existing data stores option:

Press enter or click to view image in full size

And then selecting our custom Datastore we created using SDK:

Press enter or click to view image in full size

Now, let’s test it out and ask ai agent to list my tickets:

Press enter or click to view image in full size

When we click the Sources button, we can see that the listed tickets indeed come from our custom datastore and that Agentspace has enforced access control — only my tickets are on the list.

Press enter or click to view image in full size

How do I know only my tickets are displayed? Let’s look at our original dataset. It contains 6 entries where employee_email is set to user1@gmail.com.

Press enter or click to view image in full size

Now, let me log in as user2. I can only see 4 tickets assigned to me.

Press enter or click to view image in full size

This is in line with the data in our original dataset.

Press enter or click to view image in full size

What’s Next?
In my next blog post, I’ll touch on the Agentspace activation layer. We will add a custom ADK (Agent Development Kit) agent that will use our new custom datastore and then register that ADK Agent in Agentspace. Stay tuned!

Summary
In this blog post, we explored how to extend Agentspace, Google Cloud’s new service for secure enterprise knowledge access and activation, by creating a custom indexing connector. We first distinguished between indexing connectors, which pull and store data in Agentspace, and transactional connectors, which push queries to external systems. We then walked through the key steps of building a custom connector, including:

Creating a Datastore: Setting up a secure container for our data and enabling access control (acl_enabled=True).
Mapping Data: Converting raw data from an external system (in our example, IT tickets) into a discoveryengine.Document format that includes metadata and access control rules.
Importing Documents: Using a Python script to import these documents into the Datastore.
Finally, we demonstrated how Agentspace enforces data privacy by testing the custom datastore within an application. The results showed that users could only view the tickets they were authorized to see, proving that our custom connector successfully synchronized both the data and the access control information. This foundation sets the stage for future posts on using custom datastores with Agentspace’s activation layer.

Check my other blog post to learn how to connect your ADK agent to 100+ systems with GCP Integration Connectors:

Connect & Act: Google ADK Agents with GCP Integration Connectors to Perform Tasks Across 100+…
Google just dropped their new open-source Agent Development Kit (ADK) for building multi-agent AI applications. It’s…
This article is authored by Lukasz Olejniczak — Customer Engineer at Google Cloud. The views expressed are those of the authors and don’t necessarily reflect those of Google.

Please clap for this article if you enjoyed reading it. For more about google cloud, data science, data engineering, and AI/ML follow me on LinkedIn.

Ai Agent
Microsoft
Artificial Intelligence
Google Cloud Platform
Generative Ai
17


1




Google Cloud - Community
Published in Google Cloud - Community
69K followers
·
Last published 1 day ago
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.


Following
Olejniczak Lukasz
Written by Olejniczak Lukasz
597 followers
·
268 following
Customer Engineer at Google Cloud


Following
Responses (1)
Sascha Heyer
Sascha Heyer
﻿

Cancel
Respond
Saurabh Pandey
Saurabh Pandey

Sep 16


Very succinct and to the point article. Thanks for sharing! In real world building custom connectors with source system having millions of data points, what other challenges do you anticipate?

1 reply

Reply

More from Olejniczak Lukasz and Google Cloud - Community
Mixing AI Agents with code blocks using Google ADK — can it be even better with FunctionAgent?
Google Cloud - Community
In

Google Cloud - Community

by

Olejniczak Lukasz

Mixing AI Agents with code blocks using Google ADK — can it be even better with FunctionAgent?
Agent Development Kit is open-source library for building production-ready, multi-agent applications. While it’s optimized for complex…
Sep 4
72
1


The Next Evolution of the Golden Layer. The Semantic Model for AI.
Google Cloud - Community
In

Google Cloud - Community

by

Cornelius van Heerden

The Next Evolution of the Golden Layer. The Semantic Model for AI.
Stop Asking AI to Read Your Data Warehouse. It Can’t.
Sep 14
109
3


Build a GCP Cost Agent with ADK and MCP Toolbox for Databases to Analyze Your Cloud Spending
Google Cloud - Community
In

Google Cloud - Community

by

Aryan Irani

Build a GCP Cost Agent with ADK and MCP Toolbox for Databases to Analyze Your Cloud Spending
Cloud costs are one of the hardest things to keep track of. Whether you’re running a single project or managing dozens across an…
Sep 26
206
2


Firebase Studio + Google Agent Development Kit (ADK) — build multi-agent apps from web IDE
Google Cloud - Community
In

Google Cloud - Community

by

Olejniczak Lukasz

Firebase Studio + Google Agent Development Kit (ADK) — build multi-agent apps from web IDE
Chances are, you’re already familiar with Firebase. It’s Google’s comprehensive platform built to simplify the entire application…
Apr 22
92
2


See all from Olejniczak Lukasz
See all from Google Cloud - Community
Recommended from Medium
Build a GCP Cost Agent with ADK and MCP Toolbox for Databases to Analyze Your Cloud Spending
Google Cloud - Community
In

Google Cloud - Community

by

Aryan Irani

Build a GCP Cost Agent with ADK and MCP Toolbox for Databases to Analyze Your Cloud Spending
Cloud costs are one of the hardest things to keep track of. Whether you’re running a single project or managing dozens across an…
Sep 26
206
2


New on GCP: BigQuery MCP Server (Example use ADK code)
Writing in the World of Artificial Intelligence
In

Writing in the World of Artificial Intelligence

by

Abish Pius

New on GCP: BigQuery MCP Server (Example use ADK code)
For AI agents to be truly useful in the enterprise, they need more than just access to data — they need the ability to securely analyze…

Oct 2
2


Simple custom ADK Agent (text + image) deployed to GCP Agent Engine and executed from GCP Gemini…
Jeremy Toussaint
Jeremy Toussaint

Simple custom ADK Agent (text + image) deployed to GCP Agent Engine and executed from GCP Gemini…
Before we dive into the steps this post will cover, let's define the tools in this architecture and how they all work together.
Jul 29
5
1


Introducing Conversational Analytics, Custom Agents and Code Interpreter in Looker and Looker…
Rittman Analytics Blog
In

Rittman Analytics Blog

by

Mark Rittman

Introducing Conversational Analytics, Custom Agents and Code Interpreter in Looker and Looker…
May 12
13
3


Deploying AI agents with Google ADK and Vertex AI Agent Engine
Michaël Scherding
Michaël Scherding

Deploying AI agents with Google ADK and Vertex AI Agent Engine
Building an AI agent that works on your laptop is one thing. Deploying it to production with enterprise-grade scalability, session…
Sep 22
11


Structured Outputs in Google ADK — Part 3 of the Series
Dharmendra Pratap Singh
Dharmendra Pratap Singh

Structured Outputs in Google ADK — Part 3 of the Series
Recap of Previous Articles
Aug 8
56


See more recommendations
Help

Status

About

Careers

Press

Blog

Privacy

Rules

Terms

Text to speech
Sidebar menu
Search
Write
Notifications1

Sascha Heyer
Home
Library
Profile
Stories
Stats
Following
The Startup
The Startup
Towards AI
Towards AI
Artificial Intelligence in Plain English
Artificial Intelligence in Plain English
AWS in Plain English
AWS in Plain English
Muzli
Muzli
Entrepreneurship Handbook
Entrepreneurship Handbook
Google Cloud - Community
Google Cloud - Community
Data Science Collective
Data Science Collective
Mayurkumar Surani
Mayurkumar Surani
UX Collective
UX Collective

More
Google Cloud - Community
Google Cloud - Community
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.

Following

Gemini Enterprise Handbook: A Unified, Secure Agentic Platform for Enterprise Data Grounding and AI Agent Management
Zelda Ailine Luconi
Zelda Ailine Luconi

Follow
20 min read
·
4 days ago
11






Almost two week, Google Cloud has introduced Gemini Enterprise, positioning it as the new, integrated front door for Artificial Intelligence (AI) within the workplace.

Going beyond the limitations of simple chatbots, this platform is an advanced agentic solution designed to bring the full power of Google’s AI to every employee to seamlessly discover, create, share, and run AI agents all within a single, secure environment.

If you haven’t hear about it yet, or you had no time to study the official documentation, here is a handbook for you ❤️ (including links to the official documentation)!

Press enter or click to view image in full size

From AgentSpace to Gemini Enterprise
The foundational technology powering Gemini Enterprise has a clear lineage: the conversational AI and agent creation technology previously known as Google Agentspace, which is now part of Gemini Enterprise.

Gemini Enterprise, however, significantly builds upon this foundation:

It combines this core technology with Google DeepMind’s cutting-edge AI models, notably the industry-leading Gemini models (Model Integration).
It incorporates specialized, Google-built agents and offers broad connectivity across company data (Pre-built Agents).
The combination results in a single, secure agentic platform designed for enterprise search, analysis, and synthesis (Unified Platform).
Notice: Existing Agentspace customers can continue using their current product without changes to generally available features, billing, or support, but they can contact Google Cloud sales for an upgrade to Gemini Enterprise.

⚠️Disclaimer⚠️
Before we dive in, I know this article is quite long! Don’t worry, I won’t be offended if you want to skip ahead. Feel free to jump to the section that interests you most.

I. Gemini Enterprise Architecture explains key architectural components like Apps and Data Stores, discussing data gathering via Connectors, and outlining the process for custom data ingestion and enabling agents to perform actions.
II. A Taskforce of AI Agents and Creators introduces the platform’s core agentic capabilities, detailing the Ready-to-Use Google Agents and the methods for Custom Agent Creation using the no-code Agent Designer or the developer-focused Agent Development Kit (ADK).
III. Security and Governance is an extensive section covering critical security features such as Access Control through strict ACLs and identity mapping, Networking controls including VPC Service Controls (VPC SC) and Private Service Connect (for hybrid/multi-cloud data), Model Armor, and support for Customer-Managed Encryption Keys (CMEK) for granular data sovereignty.
IV. Observability focuses on monitoring and auditing, utilizing the Google Cloud Observability suite.
V. Gemini Enterprise Tiers send you back to the official documentation to differentiate the product offerings and its key features.
Gemini Enterprise Editions Notice: In this article, I am describing Gemini Enterprise as a whole, however, different editions are available with different limitations, please check the official documentation, to be sure which feature is covered by which edition.

Gemini Enterprise Architecture
For agents to be effective, they must be grounded in the company’s reality, which means in the era of Big Data that you have to fuel Gemini Enterprise with what had been described in The Economist’s 2017 article “The world’s most valuable resource is no longer oil, but data”. Aka your data.

Connectors
To gather the new data-petrol, Gemini enterprise uses connectors (similarly to Agent Space) either through ingestion (indexing), which copies data into the index for improved search quality, or through data federation, which retrieves information directly from the source without copying it.

Gemini Enterprise offers prebuilt connectors to ingest data from both Google and third-party applications:

Google Sources: BigQuery, Cloud Storage, Google Drive, Gmail, Google Sites, Google Calendar, Cloud SQL, Spanner, Firestore, Bigtable, and AlloyDB for PostgreSQL.
Third-Party Sources: Connectors include Jira Cloud, Confluence Cloud, Microsoft Entra ID, Microsoft OneDrive, Microsoft Outlook, Microsoft SharePoint Online, ServiceNow, Box, Salesforce, and Slack.
But Gemini also provides clear mechanisms, documentation, and specific API methods (like SetUpDataConnector) and resource creation workflows (e.g., identity mapping stores and data imports) to establish connections for custom data sources or self-hosted applications.

However, when designing, consider that each data source can contain only one type of data: Structured data (e.g., table rows, JSON records following a schema) or Unstructured data (e.g., HTML, PDF, TXT, with PPTX and DOCX available in Preview).

Apps, data stores, blended search, actions and agents
Now, lets define who consumes data ingested with the connectors and how.

Press enter or click to view image in full size

Gemini Enterprise Architecture
Your data consumer is a Gemini Enterprise App (interchangeable with engine in API context) which provides search results, actions, and agents to your end users. This app may be connected to one or more Data Stores (*), which are entities containing ingested data from Google data sources (like Gmail, Calendar) or third-party applications (like Confluence, Outlook).

(*) Notice: the capability of connecting multiple data stores to a single app, enabling search across various sources and data types is called blended search feature.

The App however acts more as the central hub (or the secure environment for the end-user interaction), and when it comes to addressing a user promt, what it does is redirect towards Agent is a specific piece of AI functionality deployed within that hub to perform a defined task, including Actions (like sending an email).

An Example: IT Resolution Hub
If you are still a bit confused about terminology or what is the difference between an app and an agent, think this way.

A global technology firm leverages Gemini Enterprise to accelerate its IT support workflows. The administrator first creates a central App (or engine) named “IT Resolution Hub,” which is the end-user interface providing search, answers, and access to specialized agents.

This App requires access to three critical data sources: structured support tickets, unstructured technical documentation, and employee communication history. To achieve this, the administrator sets up several data stores. The first data store connects directly to the internal technical documentation repository (unstructured data in PDF and DOCX format). The second and third data stores are linked via pre-built third-party connectors (which are synonymous with data stores for third-party applications), specifically utilizing the ServiceNow connector for incident data and the Jira Cloud connector for bug tracking data.

Because the “IT Resolution Hub” App links to multiple data stores — ServiceNow, Jira, and internal documents — it is configured for blended search, allowing users to query all sources simultaneously for a comprehensive answer.

When an IT analyst queries the App with a complex issue (“Why is the ‘Submit’ button failing on the latest release?”), the search performs a correlated search across all sources, returning a synthesized answer grounded in both structured Jira tickets and unstructured technical manuals.

If the analyst needs deep analysis on historical failures, they can access a specialized agent via the App. For example, they deploy the Deep Research agent, which autonomously reasons and executes hundreds of searches across the entire connected corpus to generate a report on similar past incidents, drastically reducing manual research time.

Crucially, the Gemini Enterprise App also enables actions, allowing the assistant to perform tasks outside of pure search. After reviewing the agent’s report, the analyst can tell the assistant, “Create a critical incident ticket about this failure and assign it to the core engineering team.” Since the administrator configured the ServiceNow actions (available in Preview and requiring Plus licenses) via its connector setup, the assistant executes the complex API call, confirming the creation of the ServiceNow incident ticket (e.g., INC0010001) without the analyst ever leaving the conversational interface. Similarly, the analyst can request the assistant to “send an email to the team lead with the incident number INC0010001” by leveraging Gmail actions.

This integration of data, agents, and executable actions transforms complex multi-step processes into single conversational prompts.

Hopefully, now you have a good grasp of the Gemini Enterprise Architecture and you are ready for deep-diving into a costumed data ingestion.

Custom connectors and Data storing
If your enterprise has custom applications built in-house, you must first prepare the data, so that data is accessible and correctly formatted in a first-party Google data source, such as: Cloud Storage, BigQuery, Google Drive, but even local JSON . To cut it short, it means that you have to set up an Ingestion process, this can be done in with periodic syncing or with one-time import.

Once the data is staged in GCP, you create a Data Store (keep in mind, one data type per data source, either structured or unstructured) in Gemini Enterprise and configure it to pull the data. This ingestion can happen via two primary methods that cover both batch and real-time needs:

Data is imported or refreshed by using the console.
Data can be imported (upserted or deleted) using CRUD(Create, Read, Update, Delete) methods via the REST API. For periodic updates, you use the reconciliationMode parameter during import, choosing between INCREMENTAL (adds new or replaces existing documents) or FULL (replaces the entire data store content).
Data can be imported using python scripts
Remember that, when it comes to schema definition for custom structured data, you can either let Gemini Enterprise auto-detect the schema from the ingested data or explicitly provide your own schema as a JSON object via the API. You can refresh the data in a structured data store as long as you use a schema that is the same or backward compatible with the schema in the data store. For example, adding only new fields to an existing schema is backward compatible.

Taskforce of AI Agents and Creators
Gemini Enterprise serves as a central hub, offering centralized visibility and control over all AI agents used within the organization, whether they are created by Google, third-party partners, or internal teams. Therefore, it is now time to discuss about Agents.

1. Ready-to-Use Google Agents
Gemini Enterprise includes a “taskforce” of specialized, pre-built agents designed to deliver immediate value across business functions:

Deep Research: This agent is capable of complex reasoning and planning, performing hundreds of searches across the web and enterprise access-controlled data to generate a comprehensive report, drastically slashing research time from weeks to hours.
Data Insights (Preview): This tool provides actionable insights by analyzing BigQuery data, eliminating the need for users to have prior SQL knowledge.
NotebookLM Enterprise: An AI-powered research and writing tool designed to summarize and extract crucial information from dense, complex sources, functioning as a knowledge assistant.
Gemini Code Assist Standard: A specialized agent focused on helping developers complete coding tasks throughout the software development lifecycle.
2. Empowering Custom Agent Creation
As said before, you can not only use the ready-to-use agents, but also build your own ones, and it doesn’t matter if you are a developer or not, because Gemini Enterprise has two options:

Agent Designer (No-Code): This non-coding tool enables every employee (from marketing to finance) to transform their unique expertise into scalable, automated “AI helpers”.
Agent Development Kit (ADK): This is toolkit provided by Google, designed to help developers build AI agents, that are capable of reasoning, remembering information, and interacting with each other (an innovative multiple agent approach compared to other similar tools). The agents follow Google’s Agent Protocol, which means agents from different companies can interact safely, and once build within Vertex AI, you can deploy and govern them within the Gemini Enterprise platform.
3. Third-party partners’ Agents
If you don’t want to waste time reinventing the wheel or you don’t feel ready to develop your own agent, consider checking Agent finder to discover agents which match your specific use cases.

Security and Governance
As with great power comes great responsibility, the accumulation of massive datasets demands the implementation of equally stringent security protocols and access controls. Therefore, let’s see how you can governate and secure Gemini Enterprise (or better how it secure your apps, agent and data), in particular speaking about Access, Network, Model Armor, and Encryption.

Access: Granular Data Sovereignty
No matter if you are build a custom connector or not, an app or one thousand of them, it is important to define access, and Gemini Enterprise handles this through a centralized governance framework built on strict identity and access controls (ACLs).

To ensure end users only view documents and resources they are authorized to access, Google uses your configured Identity Provider (IDP) — either Google Identity for Google Workspace sources or Workforce Identity Federation (WIF) for third-party sources like Microsoft Entra ID — to authenticate the user and determine their permissions.

Furthermore, for organizations using custom data sources with internal, application-specific user groups (referred to as external identities), administrators must create an identity mapping store to map the IDP identities to these external identities, ensuring accurate ACL enforcement for custom connector data stores.

This ensures that all generative answers and specialized agents — including Google-made agents like Deep Research — operate strictly across enterprise access-controlled data, so that example

Networking
What about network then? Gemini Enterprise is built on the robust Google Cloud infrastructure and offers critical networking and security controls to handle connectivity to both cloud and self-hosted data sources, ensuring data security and compliance for enterprise users.

To do so, there are several mechanisms that address enterprise networking challenges, particularly concerning hybrid and multi-cloud environments:

Google’s network and VPC Firewall rules for Third-Party Connectors and External Endpoints
VPC Service Controls for apps and data stores residing within Google Cloud
Private Service Connect for Self-Hosted and Hybrid/Multi-Cloud Sources
Third-Party Connectors and External Endpoints: When using third-party connectors (like those for ServiceNow or Salesforce), the platform handles external network interactions. Since third-party connectors interact with public endpoints outside Google’s network (e.g., APIs for polling data or webhooks), Gemini Enterprise ensures that egress traffic is secured through granular VPC Firewall rules, which restrict outbound connections exclusively to the Fully Qualified Domain Names (FQDNs) of the external service provided by the customer.

Securing Google Cloud Resources with VPC Service Controls (VPC SC): For apps and data stores residing within Google Cloud, Gemini Enterprise integrates also with VPC Service Controls to establish a secure service perimeter. Therefore, VPC SC is crucial for mitigating the risk of data exfiltration by protecting and controlling access to the Gemini Enterprise app and its connected enterprise data. However, it is important to note that when VPC Service Controls are enabled, the creation and use of assistant actions (like sending an email or creating a Jira ticket) are blocked by default, as these are considered potential paths for data to leave the secure perimeter (To enable specific actions, the relevant services must be added to an allow list by contacting a Google representative). Kepp also in mind that VPC Service Controls can be used alongside Access Context Manager to gate public access to Gemini Enterprise applications, adding additional control beyond default authentication and authorization.

Connecting to Self-Hosted and Hybrid/Multi-Cloud Sources (Private Service Connect): For enterprise data that resides outside of the Google Cloud managed environment — such as on-premises data centers, Google Kubernetes Engine (GKE) clusters, Compute Engine VMs, or other cloud providers (like AWS or Azure) — Gemini Enterprise utilizes Private Service Connect, which establishes a secure and scalable communication channel that bypasses the public internet, minimizing security risks associated with traditional network configurations and ensuring sensitive data stays within the customer’s control and network boundaries. In this connectivity model, Gemini Enterprise acts as the service consumer, and the customer’s network (containing load balancers referencing the self-hosted resources) acts as the service producer. Notice: When setting up Private Service Connect for Gemini Enterprise, administrators must enable global access when creating the internal load balancer forwarding rule, as Gemini Enterprise may not be available in all locations.

Model Armor: Proactive AI Security Screening
Have you have hear of Cloud Armor? Well if you haven’t, Google Cloud Armor is a security service offered by Google Cloud that provides distributed denial-of-service (DDoS) protection and Web Application Firewall (WAF) capabilities.

Similarly for AI applications, you have Model Armor which operates by proactively screening both user prompts and the responses given by the Gemini Enterprise assistant to protect against various risks and ensures responsible AI practices. Notice: Model Armor is available on all Gemini Enterprise editions at no additional cost.

Press enter or click to view image in full size

Model Armor Architecture (simplified)
Model Armor performs functions directly related to 1) Safety Filtering 2) Data Governance and Compliance 3) Input/Prompt Injection Defenses

Safety Filtering — To enhance the security and safety of your AI applications Model Armor works by:

by proactively screening the prompts and responses given by the Gemini Enterprise assistant. This screening process helps protect against various risks and ensures responsible AI practices.
by applying response blocking. The system’s response to potential issues in queries or responses is governed by an enforcement type, and if you set it to “Inspect and block” (which is the default when creating a template using the console), Gemini Enterprise blocks the request and displays an error message if a policy violation is detected.
Data Governance and Compliance — Model Armor contributes by securing the interaction stream and providing auditable records of the security process (while, as you remember, the high-level policy of data usage is governed by the specific Gemini Enterprise edition). When Gemini Enterprise is configured to use Model Armor, the effective compliance certifications are the common subset of both products. (Notice: Google recommends reviewing both Gemini Enterprise and Model Armor certifications to ensure they meet regulatory requirements). In addition, Model Armor can write Data Access audit logs and these logs analyze and report on the request and response screening verdicts generated by Model Armor. These audit logs do not contain the actual user queries or assistant responses but record screening decisions, making them safe for reporting and analytics and in any case Google recommends logs should be rerouted to a secure storage destination like BigQuery, which offers stricter access controls, rather than configuring Cloud Logging directly in the Model Armor template for Gemini Enterprise apps, as this could expose sensitive data.

Input/Prompt Injection Defenses — Model Armor acts as a defense mechanism against malicious inputs, specifically targeting the prompts users send to the model.

Prompt Screening: Model Armor’s core function is to proactively screen user prompts. If the screening service is configured to Inspect and block, it will prevent the execution of queries or inputs that violate the policy.
Handling Malicious Content: If a Model Armor template is configured to screen user requests, and a document included in the request violates the policies, that document is discarded and isn’t included in the request. This prevents attempts to influence the model’s behavior or inject harmful data via uploaded content.
Failure Control: When the Model Armor screening service is unavailable (e.g., due to processing failures), administrators can configure Gemini Enterprise to intentionally “Block all user interactions” (Fail Closed mode). This proactive blocking mode ensures that potentially malicious or unscreened requests are not processed, serving as a robust final defense against prompt risks during service interruptions.
While Google Cloud Armor protects the network infrastructure, the features you described as Model Armor protect the AI model and its data/interactions, which it’s a great way to think about the different layers of security needed for modern cloud applications!

Encryption Keys
If Google default encryption (primarily a AES-256 for data at rest and a TLS for data in transit) is not enough, since maybe you have more stringent security and sovereignty requirements, the Gemini Enterprise supports Customer-Managed Encryption Keys (CMEK) in Cloud KMS, which offers granular control over data encryption for data at rest.

In particular, CMEK protection extends beyond the data stores themselves to also cover other app-owned core information, such as session data generated during searches with follow-ups, provided the associated data stores are CMEK-protected.

And remember, using CMEK keys gives customers control over:

Protection level: You can control the cryptographic strength and type of key used for encryption, such as a symmetric or asymmetric key.
Location: You can specify the geographic location where your key is stored and used, which is important for meeting data domiciling or locality requirements.
Rotation schedule: You can set up automatic key rotation, which involves creating a new key version for the same key ring. This is a security best practice that limits the amount of data encrypted by a single key version.
Usage permissions: You can manage who can use the key by granting or revoking access through the use of roles and permissions. For example, you can grant the cloudkms.cryptoKeyEncrypterDecrypter role to a service account, allowing it to encrypt and decrypt data, and then revoke this role to deny access.
Key lifecycle management: You can control the entire lifecycle of the key, including creating, enabling, disabling, and destroying it. If you disable or destroy a key, you can also revoke access to all data encrypted by that key.
Audit logs: Cloud providers maintain audit logs of all key usage, allowing you to monitor who is using your keys and when.
If you prefer, Gemini Enterprise also supports External Key Manager (EKM) or Hardware Security Module (HSM) in combination with CMEK, although EKM usage is currently in GA with an allowlist.

An Example:
Let’s use an example to show the interaction of all these components, but consider that a Secure Network Perimeter, the use of Customer-Managed Encryption Keys (CMEK) and Cloud Armor are not always required.

Secure Network Perimeter (Optional): Before any query is processed, the entire interaction must occur within your secure network boundary, enforced by VPC Service Controls. This acts as the first line of defense, creating a virtual perimeter that prevents data exfiltration and ensures that all communication between your services and Gemini Enterprise happens over a private, trusted channel.
User Authentication: An end-user submits a query from within the secure network and the first step is to authenticate the user’s identity against your company’s official Identity Provider (IDP).
Identity Federation: If needed, Gemini Enterprise uses Workforce Identity Federation (WIF) to securely accept identities from third-party IDPs like Microsoft Entra ID or Okta to enforce your existing identity policies without managing separate credentials for the AI.
Prompt Shielding (Optional): Before the query is even processed by the Gemini model, Model Armor inspects the prompt. It’s looking for malicious attempts to “jailbreak” the model, extract sensitive information about the model’s architecture, or generate harmful content. It acts as a proactive gatekeeper for the model’s input.
Permission Check: The user’s verified identity is then checked against the Access Control Lists (ACLs) on the underlying data sources. The system asks, “For this specific user, what files, rows, or records are they allowed to see?”
Identity Mapping (For Custom Sources): If you’re using a custom connector for an internal application, the Identity Mapping Store acts as a translator. It maps the user’s corporate identity (e.g., s.jones@company.com) to their application-specific username (e.g., s_jones_crm), ensuring the correct permissions are enforced.
Encrypted & Filtered Data Retrieval: The system retrieves only the data that the user is explicitly authorized to access. This data, while stored at rest in Google Cloud, may be protected by Customer-Managed Encryption Keys (CMEK) or other encryption keys.
Secure & Shielded Response Generation: Finally, the Gemini agent generates an answer based only on the pre-filtered, permission-aware data. During this process, Model Armor (Optional) provides an additional layer of protection for the model itself, shielding it against extraction and misuse, ensuring the integrity of the generative process.
The Power of Openness: Collaborative AI
While robust security is non-negotiable, Gemini Enterprise platform is also built on a principle of openness, to enable agents to communicate and conduct secure transactions regardless of the underlying model or platform, supporting standard protocols like the Agent2Agent Protocol (A2A) and Agent Payments Protocol (AP2), recognizing that the most powerful AI solutions will emerge from collaboration, not isolation.

Press enter or click to view image in full size

Agent2Agent Protocol (A2A)
This commitment is embodied by supporting open standards for agent-to-agent communication and transaction:

Agent2Agent Protocol (A2A) - The Universal Language of Agents: A2A is an open communication standard that allows different AI agents to collaborate seamlessly, irrespective of their underlying model (e.g., Gemini, GPT, custom models) or the platform they run on. Example: Your internal Gemini Enterprise agent needs highly specialized information that resides with an external logistics agent managed by a third-party vendor. A2A allows your agent to securely send a request and receive a response in a standardized format, eliminating complex, bespoke integrations. It’s the SMTP for AI agents, enabling secure, cross-platform conversations.
Agent Payments Protocol (AP2) - The Economy of Agents: AP2 is an open protocol that enables secure, trusted payments between AI agents. This moves beyond mere communication to facilitate a dynamic, on-demand economy where agents can buy and sell services. Example: Your Gemini financial analysis agent requires a real-time market sentiment analysis that’s best provided by a specialized external AI agent. Using AP2, your agent can securely request this analysis and automatically pay for the service upon delivery, transforming complex micro-transactions into seamless, automated flows.
Final consideration
Gemini Enterprise is designed to help customers meet strict compliance requirements and is backed by security measures designed for trust.

The platform supports stringent compliance needs, including but not limited to HIPAA (Health Insurance Portability and Accountability Act) and FedRAMP High-compliant which demonstrates that the platform meets the highest level of security controls required to safeguard the US government’s most sensitive, unclassified data.

Observability
Last but not least (and central to security and operation), now we speak about logging, monitoring and observability.

The Gemini Enterprise provides an interactive Analytics dashboard experience powered by Looker to get insight into the usage trends, search quality, and end-user engagement of app.

Press enter or click to view image in full size

Analytics dashboard
In addition, Gemini Enterprise incorporates robust logging and monitoring capabilities by utilizing the comprehensive Observability suite, an integrated approach that ensures administrators have the necessary tools to detect, investigate, and respond to operational issues, security threats, and usage trends across their AI deployments.

For foundational operational security and auditing, Gemini Enterprise relies heavily on detailed logging systems, namely Cloud Logging, to monitor errors and warnings related to key operational processes, specifically when importing documents or working with data connectors.
For deeper investigation, logs can be accessed through the Logs Explorer or exported to a long-term sink such as BigQuery for more complex analysis, which is also recommended by Google to ensure stricter access controls over sensitive log data.
Furthermore, Gemini Enterprise integrates Audit Logging, generating logs that record administrative and access activities within Google Cloud resources. These audit logs categorize actions into Admin Activity (for administrative changes, such as creating or deleting data stores and engines) and Data Access (for operations involving data reads or writes, such as searching or importing documents). This audited record ensures transparency and non-repudiation for governance purposes.
As said before, crucial to security observability is also the integration of Model Armor, which writes Data Access audit logs that analyze and report on the request and response screening verdicts generated during interactions with the assistant.
Finally, for organizations requiring the highest level of accountability, Access Transparency provides additional logs that specifically capture actions taken by Google personnel when accessing customer content. Notice: Access Transparency requires the app and data stores to be configured in multi-regions (US or EU), and it does not cover data associated with preview features or search analytics data.
Gemini Enterprise Tiers
As mentioned Gemini Enterprise is structured into several editions designed to meet the varying scale, complexity, and security needs of businesses, from small teams to large, heavily regulated corporations. Considering how quickly they tiers and their features change, please read the official documentation.

How to turn it on and turn it off
Gemini Enterprise is built upon the Discovery Engine API within Google Cloud, therefore, controlling the platform’s operation is managed through the API status.

Required APIs for Use: In addition to purchasing a tier model, to start utilizing Gemini Enterprise, several foundational Google Cloud APIs must be enabled in your project, including the Vertex AI API, the Gemini Enterprise (Discovery Engine) API, the Cloud Storage API, and the Identity and Access Management API.

Turning Off Gemini Enterprise: Turning off Gemini Enterprise stops billing and there is no guarantee that your data related to Gemini Enterprise will persist. However, if you delete the project, then all data is deleted according to the deletion policy for projects.

Finally a Conclusion
In conclusion, Gemini Enterprise represents the critical shift from fragmented AI tools to a unified enterprise AI fabric. 🌐

This platform delivers a comprehensive solution by combining Google’s frontier Gemini models with accessible development tools like the no-code Agent Designer 🛠️. Crucially, it provides a trusted foundation, embedding high-level security and data governance — from VPC Service Controls and CMEK to proactive Model Armor 🛡️ — directly into the architecture.

Don’t let complexity slow your strategy💡. By providing this complete, trusted, and optimized platform, Google is not just offering new tools; it is providing the unified vehicle 🚀 for enterprises to rapidly achieve tangible business value and drive deep transformation across every role, from sales to engineering.

The time for pilots is over; the time for production is now! And if they have doubts or don’t know how to use Gemini Enterprise, share this article with them! 🔗

⚠️ Please Note⚠️: Technology evolves rapidly. While this information is current as of its publication date, some details and links may become outdated over time.

Gemini
Google Cloud Platform
AI
Gemini Enterprise
Google
11





Google Cloud - Community
Published in Google Cloud - Community
69K followers
·
Last published 1 day ago
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.


Following
Zelda Ailine Luconi
Written by Zelda Ailine Luconi
13 followers
·
9 following
Cloud and Data Architect @Datwave (Google Partner), passionate about making the cloud and data landscape accessible with practical applications.


Follow
No responses yet
Sascha Heyer
Sascha Heyer
﻿

Cancel
Respond
More from Zelda Ailine Luconi and Google Cloud - Community
Stop Copying and Pasting: How to Fully Automate Google Sheets with One Script 🤯
Google Cloud - Community
In

Google Cloud - Community

by

Zelda Ailine Luconi

Stop Copying and Pasting: How to Fully Automate Google Sheets with One Script 🤯
The Power of the Dynamic Master Template
Oct 7
9


The Next Evolution of the Golden Layer. The Semantic Model for AI.
Google Cloud - Community
In

Google Cloud - Community

by

Cornelius van Heerden

The Next Evolution of the Golden Layer. The Semantic Model for AI.
Stop Asking AI to Read Your Data Warehouse. It Can’t.
Sep 14
109
3


Build a GCP Cost Agent with ADK and MCP Toolbox for Databases to Analyze Your Cloud Spending
Google Cloud - Community
In

Google Cloud - Community

by

Aryan Irani

Build a GCP Cost Agent with ADK and MCP Toolbox for Databases to Analyze Your Cloud Spending
Cloud costs are one of the hardest things to keep track of. Whether you’re running a single project or managing dozens across an…
Sep 26
206
2


FinOps & GCP Labels: From Cloud Chaos to Crystal-Clear Clarity (Gentle Introduction)
Zelda Ailine Luconi
Zelda Ailine Luconi

FinOps & GCP Labels: From Cloud Chaos to Crystal-Clear Clarity (Gentle Introduction)
In this article, we’ll embark on a journey 🚀 to understand how FinOps and labels work together to transform your GCP experience. We’ll…
Apr 9


See all from Zelda Ailine Luconi
See all from Google Cloud - Community
Recommended from Medium
Data! Data! Data! Solving Database Mysteries with Gemini CLI
Google Cloud - Community
In

Google Cloud - Community

by

MCP Toolbox for Databases

Data! Data! Data! Solving Database Mysteries with Gemini CLI
Support for your software development lifecycle using Gemini CLI extensions and the GitHub MCP server
Oct 16
292
1


Building an Agentic Deep-Thinking RAG Pipeline to Solve Complex Queries
Level Up Coding
In

Level Up Coding

by

Fareed Khan

Building an Agentic Deep-Thinking RAG Pipeline to Solve Complex Queries
Planning, Retrieval, Reflection, Critique, Synthesis and more

4d ago
798
4


Adding Empathy to Agentic AI
AI Advances
In

AI Advances

by

Debmalya Biswas

Adding Empathy to Agentic AI
Fine-tuning AI Agents based on User Personas to improve their Empathy Quotient

5d ago
336
6


Claude Skills: The AI Feature That Actually Solves a Real Problem
Pawel
Pawel

Claude Skills: The AI Feature That Actually Solves a Real Problem
Yesterday, Anthropic quietly released what might be the most practical AI feature of 2025. It’s not flashier models or better benchmarks…
Oct 17
177
8


I Spent $200 on Claude Last Month. Then I Found GLM-4.6
AI Mind
In

AI Mind

by

Adham Khaled

I Spent $200 on Claude Last Month. Then I Found GLM-4.6
How Z.ai’s new 355B parameter model delivers enterprise-grade coding at 1/7th the cost — and why embedded engineers like me are switching

Oct 14
304
9


The Self-Hosted Alternative to Google Notebook LM Is Here.
Bytefer
Bytefer

The Self-Hosted Alternative to Google Notebook LM Is Here.
Google Is Reading Your Notes. Here’s the 2-Minute Fix to Take Back Your Privacy.

4d ago
140
3


See more recommendations
Help

Status

About

Careers

Press

Blog

Privacy

Rules

Terms

Text to speechSkip to main content
Google Cloud
Documentation
Technology areas

Cross-product tools

Related sites


Search
/


English
Console

Gemini Enterprise
Configure Gemini Enterprise
Use Gemini Enterprise
NotebookLM Enterprise
Reference
Resources
Contact Us
Filter

AI and ML
Gemini Enterprise
Send feedbackRefresh structured and unstructured data
This page describes refreshing structured and unstructured data.

Refresh structured data
You can refresh the data in a structured data store as long as you use a schema that is the same or backward compatible with the schema in the data store. For example, adding only new fields to an existing schema is backward compatible.

You can refresh structured data in the Google Cloud console or using the API.

Console
REST
Python
Before trying this sample, follow the Python setup instructions in the Gemini Enterprise quickstart using client libraries. For more information, see the Gemini Enterprise Python API reference documentation.

To authenticate to Gemini Enterprise, set up Application Default Credentials. For more information, see Set up authentication for a local development environment.


from google.api_core.client_options import ClientOptions
from google.cloud import discoveryengine

# TODO(developer): Uncomment these variables before running the sample.
# project_id = "YOUR_PROJECT_ID"
# location = "YOUR_LOCATION" # Values: "global"
# data_store_id = "YOUR_DATA_STORE_ID"
# bigquery_dataset = "YOUR_BIGQUERY_DATASET"
# bigquery_table = "YOUR_BIGQUERY_TABLE"

#  For more information, refer to:
# https://cloud.google.com/generative-ai-app-builder/docs/locations#specify_a_multi-region_for_your_data_store
client_options = (
    ClientOptions(api_endpoint=f"{location}-discoveryengine.googleapis.com")
    if location != "global"
    else None
)

# Create a client
client = discoveryengine.DocumentServiceClient(client_options=client_options)

# The full resource name of the search engine branch.
# e.g. projects/{project}/locations/{location}/dataStores/{data_store_id}/branches/{branch}
parent = client.branch_path(
    project=project_id,
    location=location,
    data_store=data_store_id,
    branch="default_branch",
)

request = discoveryengine.ImportDocumentsRequest(
    parent=parent,
    bigquery_source=discoveryengine.BigQuerySource(
        project_id=project_id,
        dataset_id=bigquery_dataset,
        table_id=bigquery_table,
        data_schema="custom",
    ),
    # Options: `FULL`, `INCREMENTAL`
    reconciliation_mode=discoveryengine.ImportDocumentsRequest.ReconciliationMode.INCREMENTAL,
)

# Make the request
operation = client.import_documents(request=request)

print(f"Waiting for operation to complete: {operation.operation.name}")
response = operation.result()

# After the operation is complete,
# get information from operation metadata
metadata = discoveryengine.ImportDocumentsMetadata(operation.metadata)

# Handle the response
print(response)
print(metadata)
Refresh unstructured data
You can refresh unstructured data in the Google Cloud console or using the API.

Console
REST
Python
Before trying this sample, follow the Python setup instructions in the Gemini Enterprise quickstart using client libraries. For more information, see the Gemini Enterprise Python API reference documentation.

To authenticate to Gemini Enterprise, set up Application Default Credentials. For more information, see Set up authentication for a local development environment.

from google.api_core.client_options import ClientOptions
from google.cloud import discoveryengine

# TODO(developer): Uncomment these variables before running the sample.
# project_id = "YOUR_PROJECT_ID"
# location = "YOUR_LOCATION" # Values: "global"
# data_store_id = "YOUR_DATA_STORE_ID"

# Examples:
# - Unstructured documents
#   - `gs://bucket/directory/file.pdf`
#   - `gs://bucket/directory/*.pdf`
# - Unstructured documents with JSONL Metadata
#   - `gs://bucket/directory/file.json`
# - Unstructured documents with CSV Metadata
#   - `gs://bucket/directory/file.csv`
# gcs_uri = "YOUR_GCS_PATH"

#  For more information, refer to:
# https://cloud.google.com/generative-ai-app-builder/docs/locations#specify_a_multi-region_for_your_data_store
client_options = (
    ClientOptions(api_endpoint=f"{location}-discoveryengine.googleapis.com")
    if location != "global"
    else None
)

# Create a client
client = discoveryengine.DocumentServiceClient(client_options=client_options)

# The full resource name of the search engine branch.
# e.g. projects/{project}/locations/{location}/dataStores/{data_store_id}/branches/{branch}
parent = client.branch_path(
    project=project_id,
    location=location,
    data_store=data_store_id,
    branch="default_branch",
)

request = discoveryengine.ImportDocumentsRequest(
    parent=parent,
    gcs_source=discoveryengine.GcsSource(
        # Multiple URIs are supported
        input_uris=[gcs_uri],
        # Options:
        # - `content` - Unstructured documents (PDF, HTML, DOC, TXT, PPTX)
        # - `custom` - Unstructured documents with custom JSONL metadata
        # - `document` - Structured documents in the discoveryengine.Document format.
        # - `csv` - Unstructured documents with CSV metadata
        data_schema="content",
    ),
    # Options: `FULL`, `INCREMENTAL`
    reconciliation_mode=discoveryengine.ImportDocumentsRequest.ReconciliationMode.INCREMENTAL,
)

# Make the request
operation = client.import_documents(request=request)

print(f"Waiting for operation to complete: {operation.operation.name}")
response = operation.result()

# After the operation is complete,
# get information from operation metadata
metadata = discoveryengine.ImportDocumentsMetadata(operation.metadata)

# Handle the response
print(response)
print(metadata)
Send feedback
Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-10-22 UTC.

Why Google
Choosing Google Cloud
Trust and security
Modern Infrastructure Cloud
Multicloud
Global infrastructure
Customers and case studies
Analyst reports
Whitepapers
Products and pricing
See all products
See all solutions
Google Cloud for Startups
Google Cloud Marketplace
Google Cloud pricing
Contact sales
Support
Community forums
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Google Cloud documentation
Code samples
Cloud Architecture Center
Training and Certification
Developer Center
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
Become a Partner
Google Cloud Affiliate Program
Press Corner
About Google
Privacy
Site terms
Google Cloud terms
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe

English
Skip to main content
Google Cloud
Documentation
Technology areas

Cross-product tools

Related sites


Search
/


English
Console

Gemini Enterprise
Configure Gemini Enterprise
Use Gemini Enterprise
NotebookLM Enterprise
Reference
Resources
Contact Us
Filter

AI and ML
Gemini Enterprise
Send feedbackCreate custom connector
This page describes how to create a custom connector.

Before you begin
Before you start, make sure you have the following:

Check whether billing is enabled on your Google Cloud project.

Install and initialize the Google Cloud CLI. Ensure it is authenticated for your project.

Obtain Discovery Engine administrator access for your Google Cloud project.

Obtain access credentials for your third-party data source (such as API keys or database authentication).

Create a clear data mapping plan. This must include which fields to index and how to represent access control including third-party identities.

Note: Custom connectors can be configured for APIs that are available in seven different languages. This documentation uses Python for all code examples, but the underlying API calls and logic are consistent across every supported language. For more information about supported languages and client libraries, see Gemini Enterprise client libraries.
Create basic connector
This section demonstrates creating a custom connector in your chosen language. The principles and patterns shown here apply to any external system. Simply adapt the API calls and data transformations for your specific source in your chosen language to create a basic connector.

Fetch data
To get started, retrieve data from your third-party data source. In this example, we demonstrate how to fetch posts using pagination. For production environments, we recommend using a streaming approach for large datasets. This prevents memory issues that can occur when loading all the data at once.

Python
    def fetch_posts(base_url: str, per_page: int = 15) -> List[dict]:
        #Fetch all posts from the given site.#
        url = base_url.rstrip("/") + "/wp-json/wp/v2/posts"
        posts: List[dict] = []
        page = 1
        while True:
            resp = requests.get(
                url,
                params={"page": page, "per_page": per_page},
            )
            resp.raise_for_status()
            batch = resp.json()
            posts.extend(batch)
            if len(batch) < per_page:
                break
            page += 1
        return posts
Transform data
To convert your source data into the Discovery Engine document format, structure it as shown in the following example payload. You can include as many key-value pairs as needed. For example, you can include the full content for a comprehensive search. Alternatively, you can include structured fields for a faceted search, or a combination of both.

Python
    def convert_posts_to_documents(posts: List[dict]) -> List[discoveryengine.Document]:
        # Convert WP posts into Discovery Engine Document messages.
        docs: List[discoveryengine.Document] = []
        for post in posts:
            payload = {
                "title": post.get("title", {}).get("rendered"),
                "body": post.get("content", {}).get("rendered"),
                "url": post.get("link"),
                "author": post.get("author"),
                "categories": post.get("categories"),
                "tags": post.get("tags"),
                "date": post.get("date"),
            }
            doc = discoveryengine.Document(
                id=str(post["id"]),
                json_data=json.dumps(payload),
            )
            docs.append(doc)
        return docs

Retrieve or create identity store
To manage user identities and groups for access control, you must retrieve or create an identity store. This function gets an existing identity store by its ID, project, and location. If the identity store doesn't exist, it creates and returns a new, empty identity store.

Python
    def get_or_create_ims_data_store(
        project_id: str,
        location: str,
        identity_mapping_store_id: str,
    ) -> discoveryengine.DataStore:
      """Get or create a DataStore."""
      # Initialize the client
      client_ims = discoveryengine.IdentityMappingStoreServiceClient()
      # Construct the parent resource name
      parent_ims = client_ims.location_path(project=project_id, location=location)

      try:
        # Create the request object
        name = f"projects/{project_id}/locations/{location}/identityMappingStores/{identity_mapping_store_id}"
        request = discoveryengine.GetIdentityMappingStoreRequest(
            name=name,
        )
        return client_ims.get_identity_mapping_store(request=request)
      except:
        # Create the IdentityMappingStore object (it can be empty for basic creation)
        identity_mapping_store = discoveryengine.IdentityMappingStore()
        # Create the request object
        request = discoveryengine.CreateIdentityMappingStoreRequest(
            parent=parent_ims,
            identity_mapping_store=identity_mapping_store,
            identity_mapping_store_id=identity_mapping_store_id,
        )
        return client_ims.create_identity_mapping_store(request=request)
The get_or_create_ims_data_store function uses the following key variables:

project_id: The ID of your Google Cloud project.
location: The Google Cloud location for the identity mapping store.
identity_mapping_store_id: A unique identifier for the identity store.
client_ims: An instance of discoveryengine.IdentityMappingStoreServiceClient used to interact with the identity store API.
parent_ims: The resource name of the parent location, constructed using client_ims.location_path.
name: The full resource name of the identity mapping store, used for GetIdentityMappingStoreRequest.
Ingest identity mapping into identity store
To load identity mapping entries into the specified identity store, use this function. It takes a list of identity mapping entries and initiates an inline import operation. This is crucial for establishing the user, group, and external identity relationships needed for access control and personalization.

Python
def load_ims_data(
    ims_store: discoveryengine.DataStore,
    id_mapping_data: list[discoveryengine.IdentityMappingEntry],
) -> discoveryengine.DataStore:
  """Get the IMS data store."""
  # Initialize the client
  client_ims = discoveryengine.IdentityMappingStoreServiceClient()

  #  Create the InlineSource object
  inline_source = discoveryengine.ImportIdentityMappingsRequest.InlineSource(
      identity_mapping_entries=id_mapping_data
  )

  # Create the main request object
  request_ims = discoveryengine.ImportIdentityMappingsRequest(
      identity_mapping_store=ims_store.name,
      inline_source=inline_source,
  )

  try:
    # Create the InlineSource object, which holds your list of entries
    operation = client_ims.import_identity_mappings(
        request=request_ims,
    )
    result = operation.result()
    return result

  except Exception as e:
    print(f"IMS Load Error: {e}")
    result = operation.result()
    return result
The load_ims_data function uses the following key variables:

ims_store: The discoveryengine.DataStore object representing the identity mapping store where data will be loaded.
id_mapping_data: A list of discoveryengine.IdentityMappingEntry objects, each containing an external identity and its corresponding user or group ID.
result: Return value of type discoveryengine.DataStore.
Create data store
To use a custom connector, you must initialize a data store for your content. Use the default_collection for custom connectors. The IndustryVertical parameter customizes the data store's behavior for specific use cases. GENERIC is suitable for most scenarios. However, you can choose a different value for a particular industry, such as MEDIA or HEALTHCARE_FHIR. Configure the display name and other properties to align with your project's naming conventions and requirements.

Python
def get_or_create_data_store(
    project_id: str,
    location: str,
    display_name: str,
    data_store_id: str,
    identity_mapping_store: str,
) -> discoveryengine.DataStore:
  """Get or create a DataStore."""
  client = discoveryengine.DataStoreServiceClient()
  ds_name = client.data_store_path(project_id, location, data_store_id)
  try:
    result = client.get_data_store(request={"name": ds_name})
    return result
  except:
    parent = client.collection_path(project_id, location, "default_collection")
    operation = client.create_data_store(
        request={
            "parent": parent,
            "data_store": discoveryengine.DataStore(
                display_name=display_name,
                acl_enabled=True,
                industry_vertical=discoveryengine.IndustryVertical.GENERIC,
                identity_mapping_store=identity_mapping_store,
            ),
            "data_store_id": data_store_id,
        }
    )
    result = operation.result()
    return result

The get_or_create_data_store function uses the following key variables:

project_id: The ID of your Google Cloud project.
location: The Google Cloudlocation for the data store.
display_name: The human-readable display name for the data store.
data_store_id: A unique identifier for the data store.
identity_mapping_store: The resource name of the identity mapping store to bind.
result: Return value of type discoveryengine.DataStore.
Upload documents inline
To directly send documents to Discovery Engine, use inline upload. This method uses incremental reconciliation mode by default and doesn't support full reconciliation mode. In incremental mode, new documents are added and existing ones are updated, but documents no longer in the source are not deleted. Full reconciliation mode synchronizes the data store with your source data, including deleting documents that are no longer present in the source.

Incremental reconciliation is ideal for systems like a CRM that handle frequent, small changes to data. Instead of syncing the entire database, only send specific changes, making the process faster and more efficient. A full synchronization can still be performed periodically to maintain overall data integrity.

Python
    def upload_documents_inline(
        project_id: str,
        location: str,
        data_store_id: str,
        branch_id: str,
        documents: List[discoveryengine.Document],
    ) -> discoveryengine.ImportDocumentsMetadata:
        """Inline import of Document messages."""
        client = discoveryengine.DocumentServiceClient()
        parent = client.branch_path(
            project=project_id,
            location=location,
            data_store=data_store_id,
            branch=branch_id,
        )
        request = discoveryengine.ImportDocumentsRequest(
            parent=parent,
            inline_source=discoveryengine.ImportDocumentsRequest.InlineSource(
                documents=documents,
            ),
        )
        operation = client.import_documents(request=request)
        operation.result()
        result = operation.metadata
        return result
The upload_documents_inline function uses the following key variables:

project_id: The ID of your Google Cloud project.
location: The Google Cloud location for the data store.
data_store_id: The ID of the data store.
branch_id: The ID of the branch within the data store (typically "0").
documents: A list of discoveryengine.Document objects to be uploaded.
result: Return value of type discoveryengine.ImportDocumentsMetadata.
Validate your connector
To validate that your connector is working as expected, perform a test run to ensure proper data flow from the source to Discovery Engine.

Python
    SITE = "https://altostrat.com"
    PROJECT_ID = "ucs-3p-connectors-testing"
    LOCATION = "global"
    IDENTITY_MAPPING_STORE_ID = "your-unique-ims-id17" # A unique ID for your new store
    DATA_STORE_ID = "my-acl-ds-id1"
    BRANCH_ID = "0"

    posts = fetch_posts(SITE)
    docs = convert_posts_to_documents(posts)
    print(f"Fetched {len(posts)} posts and converted to {len(docs)} documents.")

    try:
      # Step #1: Retrieve an existing identity mapping store or create a new identity mapping store
      ims_store = get_or_create_ims_data_store(PROJECT_ID, LOCATION, IDENTITY_MAPPING_STORE_ID)
      print(f"STEP #1: IMS Store Retrieval/Creation: {ims_store}")

      RAW_IDENTITY_MAPPING_DATA = [
          discoveryengine.IdentityMappingEntry(
              external_identity="external_id_1",
              user_id="testuser1@example.com",
          ),
          discoveryengine.IdentityMappingEntry(
              external_identity="external_id_2",
              user_id="testuser2@example.com",
          ),
          discoveryengine.IdentityMappingEntry(
              external_identity="external_id_2",
              group_id="testgroup1@example.com",
          )
      ]

      # Step #2: Load IMS Data
      response = load_ims_data(ims_store, RAW_IDENTITY_MAPPING_DATA)
      print(
          "\nStep #2: Load Data in IMS Store successful.", response
      )

      # Step #3: Create Entity Data Store & Bind IMS Data Store
      data_store =  get_or_create_data_store(PROJECT_ID, LOCATION, "my-acl-datastore", DATA_STORE_ID, ims_store.name)
      print("\nStep #3: Entity Data Store Create Result: ", data_store)

      metadata = upload_documents_inline(
          PROJECT_ID, LOCATION, DATA_STORE_ID, BRANCH_ID, docs
      )
      print(f"Uploaded {metadata.success_count} documents inline.")

    except gcp_exceptions.GoogleAPICallError as e:
      print(f"\n--- API Call Failed ---")
      print(f"Server Error Message: {e.message}")
      print(f"Status Code: {e.code}")

    except Exception as e:
      print(f"An error occurred: {e}")
Validate your connector code uses the following key variables:

SITE: The base URL of the third-party data source.
PROJECT_ID: Your Google Cloud project ID.
LOCATION: The Google Cloud location for the resources.
IDENTITY_MAPPING_STORE_ID: A unique ID for your Identity Mapping Store.
DATA_STORE_ID: A unique ID for your data store.
BRANCH_ID: The ID of the branch within the data store.
posts: Stores the fetched posts from the third-party source.
docs: Stores the converted documents in discoveryengine.Document format.
ims_store: The retrieved or created discoveryengine.DataStore object for identity mapping.
RAW_IDENTITY_MAPPING_DATA: A list of discoveryengine.IdentityMappingEntry objects.
Expected output:

Shell
  Fetched 20 posts and converted to 20 documents.
  STEP #1: IMS Store Retrieval/Creation: "projects/ <Project Number>/locations/global/identityMappingStores/your-unique-ims-id17"
  Step #2: Load Data in IMS Store successful.
  Step #3: Entity Data Store Create Result: "projects/ <Project Number>/locations/global/collections/default_collection/dataStores/my-acl-ds-id1"
  display_name: "my-acl-datastore"
  industry_vertical: GENERIC
  create_time {
    seconds: 1760906997
    nanos: 192641000
  }
  default_schema_id: "default_schema"
  acl_enabled: true
  identity_mapping_store: "projects/ <Project Number>/locations/global/identityMappingStores/your-unique-ims-id17".
  Uploaded 20 documents inline.
You can also see your data store in the Google Google Cloud console at this point:

Custom connector data store
Custom connector data store.
Create connector with Google Cloud Storage upload
While inline import works well for development, production connectors should use Google Cloud Storage for better scalability and to enable full reconciliation mode. This approach handles large datasets efficiently and supports automatic deletion of documents no longer present in the third-party data source.

Convert documents to JSONL
To prepare documents for bulk import into Discovery Engine, convert them to JSON Lines format.

Python
    def convert_documents_to_jsonl(
        documents: List[discoveryengine.Document],
    ) -> str:
        """Serialize Document messages to JSONL."""
        return "\n".join(
            discoveryengine.Document.to_json(doc, indent=None)
            for doc in documents
        ) + "\n"
The convert_documents_to_jsonl function uses the following variable:

documents: A list of discoveryengine.Document objects to be converted.
Upload to Google Cloud Storage
To enable efficient bulk import, stage your data in Google Cloud Storage.

Python
    def upload_jsonl_to_gcs(jsonl: str, bucket_name: str, blob_name: str) -> str:
        """Upload JSONL content to Google Cloud Storage."""
        client = storage.Client()
        bucket = client.bucket(bucket_name)
        blob = bucket.blob(blob_name)
        blob.upload_from_string(jsonl, content_type="application/json")
        return f"gs://{bucket_name}/{blob_name}"
The upload_jsonl_to_gcs function uses the following key variables:

jsonl: The JSONL formatted string content to be uploaded.
bucket_name: The name of the Google Cloud Storage bucket.
blob_name: The name of the blob (object) within the specified bucket.
Import from Google Cloud Storage with full reconciliation
To perform a complete data synchronization using full reconciliation mode, use this method. This ensures your data store mirrors the third-party data source exactly, automatically removing any documents that no longer exist.

Python
    def import_documents_from_gcs(
        project_id: str,
        location: str,
        data_store_id: str,
        branch_id: str,
        gcs_uri: str,
    ) -> discoveryengine.ImportDocumentsMetadata:
        """Bulk-import documents from Google Cloud Storage with FULL reconciliation mode."""
        client = discoveryengine.DocumentServiceClient()
        parent = client.branch_path(
            project=project_id,
            location=location,
            data_store=data_store_id,
            branch=branch_id,
        )
        gcs_source = discoveryengine.GcsSource(input_uris=[gcs_uri])
        request = discoveryengine.ImportDocumentsRequest(
            parent=parent,
            gcs_source=gcs_source,
            reconciliation_mode=
                discoveryengine.ImportDocumentsRequest
                .ReconciliationMode.FULL,
        )
        operation = client.import_documents(request=request)
        operation.result()
        return operation.metadata
The import_documents_from_gcs function uses the following key variables:

project_id: The ID of your Google Cloud project.
location: The Google Cloud location for the data store.
data_store_id: The ID of the data store.
branch_id: The ID of the branch within the data store (typically "0").
gcs_uri: The Google Cloud Storage URI pointing to the JSONL file.
Test Google Cloud Storage upload
To verify the Google Cloud Storage-based import workflow, execute the following:

Python
  BUCKET = "your-existing-bucket"
  BLOB = "path-to-any-blob/wp/posts.jsonl"
  SITE = "https://altostrat.com"
  PROJECT_ID = "ucs-3p-connectors-testing"
  LOCATION = "global"
  IDENTITY_MAPPING_STORE_ID = "your-unique-ims-id17" # A unique ID for your new store
  DATA_STORE_ID = "your-data-store-id"
  BRANCH_ID = "0"
  jsonl_payload = convert_documents_to_jsonl(docs)
  gcs_uri = upload_jsonl_to_gcs(jsonl_payload, BUCKET, BLOB)
  posts = fetch_posts(SITE)
  docs = convert_posts_to_documents(posts)
  print(f"Fetched {len(posts)} posts and converted to {len(docs)} documents.")
  print("Uploaded to:", gcs_uri)

  metadata = import_documents_from_gcs(
      PROJECT_ID, LOCATION, DATA_STORE_ID, BRANCH_ID, gcs_uri
  )
  print(f"Imported: {metadata.success_count} documents")
The following key variables are used in testing the Google Cloud Storage upload:

BUCKET: The name of the Google Cloud Storage bucket.
BLOB: The path to the blob within the bucket.
SITE: The base URL of the third-party data source.
PROJECT_ID: Your Google Cloud project ID.
LOCATION: The Google Cloud location for the resources (e.g., "global").
IDENTITY_MAPPING_STORE_ID: A unique ID for your Identity Mapping Store.
DATA_STORE_ID: A unique ID for your data store.
BRANCH_ID: The ID of the branch within the data store (typically "0").
jsonl_payload: The documents converted to JSONL format.
gcs_uri: The Google Cloud Storage URI of the uploaded JSONL file.
Expected output:

Shell
    Fetched 20 posts and converted to 20 documents.
    Uploaded to: gs://alex-de-bucket/wp/posts.jsonl
    Imported: 20 documents
Manage permissions
To manage document-level access in enterprise environments, Gemini Enterprise supports Access Control Lists (ACLs) and identity mapping, which help to limit what content users can see.

Enable ACLs in data store
To enable ACLs when creating your data store, execute the following:

Python
  # get_or_create_data_store()
  "data_store": discoveryengine.DataStore(
      display_name=data_store_id,
      industry_vertical=discoveryengine.IndustryVertical.GENERIC,
      acl_enabled=True, # ADDED
  )
Add ACLs to documents
To compute and include AclInfo when transforming the documents, execute the following:

Python
  # convert_posts_to_documents()
  doc = discoveryengine.Document(
      id=str(post["id"]),
      json_data=json.dumps(payload),
      acl_info=discoveryengine.Document.AclInfo(
          readers=[{
              "principals": [
                  {"user_id": "baklavainthebalkans@gmail.com"},
                  {"user_id": "cloudysanfrancisco@gmail.com"}
              ]
          }]
      ),
  )
Make content public
To make a document publicly accessible, set the readers field as follows:

Python
  readers=[{"idp_wide": True}]
Validate ACLs
To validate that your ACL configurations are working as expected, consider the following:

Search as a user who doesn't have access to the document.

Inspect the uploaded document structure in Cloud Storage and compare it to a reference.

JSON
  {
    "id": "108",
    "jsonData": "{...}",
    "aclInfo": {
      "readers": [
        {
          "principals": [
            { "userId": "baklavainthebalkans@gmail.com" },
            { "userId": "cloudysanfrancisco@gmail.com" }
          ],
          "idpWide": false
        }
      ]
    }
  }
Use identity mapping
Use identity mapping for the following scenarios:

Your third-party data source uses non-Google identities

You want to reference custom groups (e.g., wp-admins) instead of individual users

Your API returns only group names

You need to manually group users for scale or consistency

To do the identity mapping, follow the steps:

Create and link the identity data store.
Import external identities (For example, external_group:wp-admins). Don't include the external_group: prefix when importing, for example:

JSON
  {
    "externalIdentity": "wp-admins",
    "userId": "user@example.com"
  }
In the ACL info of your document, define the external entity ID in the principal identifier. When referencing custom groups, use the external_group: prefix in the groupId field.

The external_group: prefix is required for group IDs within the document's ACL info during import, but it is not used when importing identities to the mapping store. Example document with identity mapping:

JSON
  {
    "id": "108",
    "aclInfo": {
      "readers": [
        {
          "principals": [
            {
              "userId": "cloudysanfrancisco@gmail.com"
            },
            {
              "groupId": "external_group:wp-admins"
            }
          ]
        }
      ]
    },
    "structData": {
      "id": 108,
      "date": "2025-04-24T18:16:04",
      ...
    }
  }
Send feedback
Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-10-22 UTC.

Why Google
Choosing Google Cloud
Trust and security
Modern Infrastructure Cloud
Multicloud
Global infrastructure
Customers and case studies
Analyst reports
Whitepapers
Products and pricing
See all products
See all solutions
Google Cloud for Startups
Google Cloud Marketplace
Google Cloud pricing
Contact sales
Support
Community forums
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Google Cloud documentation
Code samples
Cloud Architecture Center
Training and Certification
Developer Center
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
Become a Partner
Google Cloud Affiliate Program
Press Corner
About Google
Privacy
Site terms
Google Cloud terms
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe

English
The new page has loaded.Skip to main content
Google Cloud
Documentation
Technology areas

Cross-product tools

Related sites


Search
/


English
Console

Gemini Enterprise
Configure Gemini Enterprise
Use Gemini Enterprise
NotebookLM Enterprise
Reference
Resources
Contact Us
Filter

AI and ML
Gemini Enterprise
Send feedbackOverview
Custom connectors let you integrate external data sources that fall outside the Gemini Enterprise standard connector library, making your organization's unique data searchable and accessible using natural language, powered by Gemini and Google's advanced search intelligence. The custom connector interacts directly with the Discovery Engine API, which enables robust data storage, indexing, and intelligent search capabilities. The connector converts the source information into the standardized JSON-based Document format (structuring the content, metadata, and Access Control Lists (ACLs)) and ensures this data is organized into Data stores. These stores act as logical repositories, ideally representing a single document format, each with its own dedicated search index and configurations.

How custom connectors work
Custom connectors work by using an automated data pipeline to perform three key actions: Fetch, Transform, and Sync. This process ensures external data is correctly prepared and uploaded to Gemini Enterprise.

Fetch: The connector pulls data, including documents, metadata, and permissions, from external system using its APIs, databases, or file formats.

Transform: The connector converts raw data into the document format of the Discovery Engine, structures the content and metadata, and assigns a globally unique ID to each document. For access controls, you can use either Google-recognized identities directly or identity mapping for external users or custom groups.

Sync: The connector uploads the documents to Gemini Enterprise data stores and keeps them updated through scheduled jobs. The data sync is performed using a data store created for an entity. For more information about creating data store, see Data store creation process. Choose a sync mode based on your needs: Incremental adds and updates data, while Full replaces the entire dataset.

ACLs and Identity mapping
To manage document-level access, choose between two methods — Pure ACLs or Identity Mapping, depending on the identity format used by the data.

Pure ACLs (AclInfo): This method is used when the data source uses email-based identities recognized by (Google Cloud). This approach is ideal for directly defining who has access.

Identity mapping: This method is used when the data source uses usernames, legacy IDs, or other external identity systems. It establishes a clear and one-to-one association between external identity groups (e.g., EXT1) and internal Identity Provider (IDP) users or groups (e.g., IDPUser1@example.com). It allows the system to understand and apply group-based access controls from the source system, which is useful when an API returns group labels without full user memberships or for efficiently scaling ACLs without listing thousands of users per document. The process requires resolving all nested or hierarchical identity structures into a flat list of direct mappings, typically in a specified JSON format. Use unique external identity group IDs (e.g., EXT1) for external identities to maintain system integrity. For more information and examples, see Identity mapping.

Data store creation process
Create the identity store: This store acts as the parent resource for all identity mappings. Upon creation, project-level Identity Provider (IDP) settings are automatically fetched. For more information, see Retrieve or create identity store.

Load external identity mappings into identity store: After creating the identity store, load the external identity data into it. For more information, see Ingest identity mapping into identity store.

Create and bind the entity data store: The entity data store can only be created after the identity store is successfully created and the identity mappings are loaded. You must bind the identity store to the entity data store during its creation. For more information about creating entity data store, see Create data store.

create-data-store-process
Data store creation process.
Data sync
There are two different architecture models for syncing data:

Architecture Model 1: Incremental upsert: The incremental upsert approach is best suited for scenarios where data is streaming and requires real-time updates. The connector leverages the Discovery Engine API to perform efficient, incremental upserts (inserting or updating data) by calling the appropriate functions with small changes as they occur. This focus on minimal change sizes and minimal delay keeps the document store highly current, even with fast-changing data.

Architecture Model 2: Comprehensive sync with Google Cloud Storage: This recommended approach offers a comprehensive set of data management features and high flexibility. It supports full syncs, which allow for data insertion, updating, and deletion across the entire dataset, and incremental syncs, which handle only inserts and updates by sending changes. This makes the approach robust for a wide range of data needs, particularly for managing larger or more complex data operations. This model utilizes a staging process (step 1 in the diagram) where the connector first writes the data to Google Cloud Storage (GCS), then leverages the Discovery Engine API to update the document store by calling the necessary import functions from the staged GCS location.

Custom connectors are flexible enough to support a hybrid architecture, allowing you to implement incremental upsert for fast-changing data and comprehensive sync for scheduled full data updates or deletions.

Custom connector data sync options
Data sync options.
Send feedback
Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-10-22 UTC.

Why Google
Choosing Google Cloud
Trust and security
Modern Infrastructure Cloud
Multicloud
Global infrastructure
Customers and case studies
Analyst reports
Whitepapers
Products and pricing
See all products
See all solutions
Google Cloud for Startups
Google Cloud Marketplace
Google Cloud pricing
Contact sales
Support
Community forums
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Google Cloud documentation
Code samples
Cloud Architecture Center
Training and Certification
Developer Center
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
Become a Partner
Google Cloud Affiliate Program
Press Corner
About Google
Privacy
Site terms
Google Cloud terms
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe

English
The new page has loaded.Skip to main content
Google Cloud
Documentation
Technology areas

Cross-product tools

Related sites


Search
/


English
Console

Gemini Enterprise
Configure Gemini Enterprise
Use Gemini Enterprise
NotebookLM Enterprise
Reference
Resources
Contact Us
Filter

AI and ML
Gemini Enterprise
Send feedbackProvide or auto-detect a schema
When you import structured data using the Google Cloud console, Gemini Enterprise auto-detects the schema. You can either use this auto-detected schema in your engine or use the API to provide a schema to indicate the structure of the data.

If you provide a schema and later update it with a new schema, the new schema must be backward compatible with the original. Otherwise the schema update fails.

Important: If you don't provide a schema, the auto-detect feature can update your schema by incorporating any newly detected fields when you import new data. If any documents in your imported data contain new fields that are not backward compatible with your original schema, those documents fail to import.
For reference information about the schema, see dataStores.schemas.

Key Terms: In the context of schema, the terms field and property are used interchangeably.
Approaches to providing the schema for your data store
There are various approaches to determining the schema for structured data.

Auto-detect and edit. Let Gemini Enterprise auto-detect and suggest an initial schema. Then, you refine the schema through the console interface. Google highly recommends that, after your fields are auto-detected, you map key properties to all the important fields.

This is the approach that you'll use when following the Google Cloud console instructions for structured data in Create a first-party data store.

Provide the schema as a JSON object. Provide the schema to Gemini Enterprise as a JSON object. You need to have prepared a correct JSON object. For an example of a JSON object, see Example schema as a JSON object. After creating the schema, you upload your data according to that schema.

This is the approach that you can use when creating a data store through the API using a curl command (or program). For example, see Import once from BigQuery. Also see the following instructions, Provide your own schema.

About auto-detect and edit
When you begin importing data, Gemini Enterprise samples the first few documents that are imported. Based on these documents, it proposes a schema for the data, which you can then review or edit.

If fields that you want to map to key properties aren't present in the sampled documents, then you can manually add these fields when you review the schema.

Tip: Make sure that the documents at the beginning of your dataset are good quality—that all the key fields are represented. That way, you don't have to add or edit fields later.
If Gemini Enterprise encounters additional fields later in the data import, it still imports these fields and adds them to the schema. If you want to edit the schema after all the data has been imported, see Update your schema.

Example schema as a JSON object
You can define your own schema using the JSON Schema format, which is an open source, declarative language to define, annotate, and validate JSON documents. For example, this is a valid JSON schema annotation:

{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "type": "object",
  "dynamic": "true",
  "datetime_detection": true,
  "geolocation_detection": true,
  "properties": {
    "title": {
      "type": "string",
      "keyPropertyMapping": "title",
      "retrievable": true,
      "completable": true
    },
    "description": {
      "type": "string",
      "keyPropertyMapping": "description"
    },
    "categories": {
      "type": "array",
      "items": {
        "type": "string",
        "keyPropertyMapping": "category"
      }
    },
    "uri": {
      "type": "string",
      "keyPropertyMapping": "uri"
    },
    "brand": {
      "type": "string",
      "indexable": true,
      "dynamicFacetable": true
    },
    "location": {
      "type": "geolocation",
      "indexable": true,
      "retrievable": true
    },
    "creationDate": {
      "type": "datetime",
      "indexable": true,
      "retrievable": true
    },
    "isCurrent": {
      "type": "boolean",
      "indexable": true,
      "retrievable": true
    }
  }
}
Here are some of the fields in this schema example:

dynamic. If dynamic is set to the string value "true", then any new properties found in the imported data is added to the schema. If dynamic is set to "false", new properties found in imported data are ignored; the properties are not added to the schema nor are the values are imported.

For example, a schema has two properties: title and description, and you upload a data that contains properties for title, description, and rating. If dynamic is "true", then the ratings property and data are imported. If dynamic is "false", then rating properties are not imported, although title and description are.

The default is "true".

datetime_detection. If datetime_detection is set to the boolean true, then, when data in datetime format are imported, the schema type is set to datetime. The supported formats are RFC 3339 and ISO 8601.

For example:

2024-08-05 08:30:00 UTC

2024-08-05T08:30:00Z

2024-08-05T01:30:00-07:00

2024-08-05

2024-08-05T08:30:00+00:00

If datatime_detection is set to the boolean false, then, when data in datetime format are imported, the schema type is set to string.

The default is true.

geolocation_detection. If geolocation_detection is set to the boolean true, then, when data in geolocation format are imported, the schema type is set to geolocation. Data is detected as geolocation if it is an object containing a latitude number and a longitude number or an object containing an address string.

For example:

"myLocation": {"latitude":37.42, "longitude":-122.08}

"myLocation": {"address": "1600 Amphitheatre Pkwy, Mountain View, CA 94043"}

If geolocation_detection is set to the boolean false, then, when data in geolocation format are imported, the schema type is set to object.

The default is true.

keyPropertyMapping. A field that maps predefined keywords to critical fields in your documents, helping to clarify their semantic meaning. Values include title, description, uri, and category. Note that your field name doesn't need to match the keyPropertyValues value. For example, for a field that you named my_title, you can include a keyPropertyValues field with a value of title.

Fields marked with keyPropertyMapping are by default indexable and searchable, but not retrievable, completable, or dynamicFacetable. This means that you don't need to include the indexable or searchable fields with a keyPropertyValues field to get the expected default behavior.

Note: Key properties can improve the quality of search results and search autocomplete accuracy. If you use auto-schema detection, key properties are not automatically added. We highly recommend using the schemas.patch method to mark data fields as key properties, especially title, uri, and description.
type. The type of the field. This is a string value that is datetime, geolocation or one of the primitive types (integer, boolean, object, array, number, or string).

retrievable. Indicates whether this field can be returned in a search response. This can be set for fields of type number, string, boolean, integer, datetime, and geolocation. A maximum of 50 fields can be set as retrievable. User-defined fields and keyPropertyValues fields are not retrievable by default. To make a field retrievable, include "retrievable": true with the field.

indexable. Indicates whether this field can be filtered, faceted, boosted, or sorted in the servingConfigs.search method. This can be set for fields of type number, string, boolean, integer, datetime, and geolocation. A maximum of 50 fields can be set as indexable. User-defined fields are not indexable by default, except for fields containing the keyPropertyMapping field. To make a field indexable, include "indexable": true with the field.

dynamicFacetable. Indicates that the field can be used as a dynamic facet. This can be set for fields of type number, string, boolean, and integer. To make a field dynamically facetable, it must also be indexable: include "dynamicFacetable": true and "indexable": true with the field.

searchable. Indicates whether this field can be reverse indexed to match unstructured text queries. This can only be set for fields of type string. A maximum of 50 fields can be set as searchable. User-defined fields are not searchable by default, except for fields containing the keyPropertyMapping field. To make a field searchable, include "searchable": true with the field.

completable. Indicates whether this field can be returned as an autocomplete suggestion. This can only be set for fields of type string. To make a field completable, include "completable": true with the field.

Note: For auto-detected schemas, newly detected fields are automatically indexable, searchable, and retrievable, as long as they follow the type requirements and stay within the maximum limits.
Provide your own schema as a JSON object
To provide your own schema, you create a data store that contains an empty schema and then you update the schema, supplying your schema as a JSON object. Follow these steps:

Prepare the schema as a JSON object, using the Example schema as a JSON object as a guide.

Create a data store.

curl -X POST \
-H "Authorization: Bearer $(gcloud auth print-access-token)" \
-H "Content-Type: application/json" \
-H "X-Goog-User-Project: PROJECT_ID" \
"https://discoveryengine.googleapis.com/v1/projects/PROJECT_ID/locations/global/collections/default_collection/dataStores?dataStoreId=DATA_STORE_ID" \
-d '{
  "displayName": "DATA_STORE_DISPLAY_NAME",
  "industryVertical": "INDUSTRY_VERTICAL"
}'
Replace the following:

PROJECT_ID: the ID of your project.
DATA_STORE_ID: the ID of the data store that you want to create. This ID can contain only lowercase letters, digits, underscores, and hyphens.
DATA_STORE_DISPLAY_NAME: the display name of the data store that you want to create.
INDUSTRY_VERTICAL: GENERIC
Use the schemas.patch API method to provide your new JSON schema as a JSON object.

curl -X PATCH \
-H "Authorization: Bearer $(gcloud auth print-access-token)" \
-H "Content-Type: application/json" \
"https://discoveryengine.googleapis.com/v1beta/projects/PROJECT_ID/locations/global/collections/default_collection/dataStores/DATA_STORE_ID/schemas/default_schema" \
-d '{
  "structSchema": JSON_SCHEMA_OBJECT
}'
Replace the following:

PROJECT_ID: the ID of your project.
DATA_STORE_ID: the ID of the data store.
JSON_SCHEMA_OBJECT: your new JSON schema as a JSON object. For example:

{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "type": "object",
  "properties": {
    "title": {
      "type": "string",
      "keyPropertyMapping": "title"
    },
    "categories": {
      "type": "array",
      "items": {
        "type": "string",
        "keyPropertyMapping": "category"
      }
    },
    "uri": {
      "type": "string",
      "keyPropertyMapping": "uri"
    }
  }
}
Example command and result
Optional: Review the schema by following the procedure View a schema definition.

What's next
Create a search app
Get the schema definition for structured data
Update a schema for structured data
Send feedback
Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-10-18 UTC.

Why Google
Choosing Google Cloud
Trust and security
Modern Infrastructure Cloud
Multicloud
Global infrastructure
Customers and case studies
Analyst reports
Whitepapers
Products and pricing
See all products
See all solutions
Google Cloud for Startups
Google Cloud Marketplace
Google Cloud pricing
Contact sales
Support
Community forums
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Google Cloud documentation
Code samples
Cloud Architecture Center
Training and Certification
Developer Center
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
Become a Partner
Google Cloud Affiliate Program
Press Corner
About Google
Privacy
Site terms
Google Cloud terms
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe

English
The new page has loaded..Skip to main content
Google Cloud
Documentation
Technology areas

Cross-product tools

Related sites


Search
/


English
Console

Gemini Enterprise
Configure Gemini Enterprise
Use Gemini Enterprise
NotebookLM Enterprise
Reference
Resources
Contact Us
Filter

AI and ML
Gemini Enterprise
Send feedbackParse and chunk documents
This page describes how to use Gemini Enterprise to parse and chunk your documents.

You can configure parsing or chunking settings in order to:

Specify how Gemini Enterprise parses content. You can specify how to parse unstructured content when you upload it to Gemini Enterprise. Gemini Enterprise provides a digital parser, OCR parser for PDFs, and a layout parser. You can also bring your own parsed documents. The layout parser is recommended when you have rich content and structural elements like sections, paragraphs, tables, images, and lists to be extracted from documents for search and answer generation.

See Improve content detection with parsing.

Use Gemini Enterprise for retrieval-augmented generation (RAG). Improve the output of LLMs with relevant data that you've uploaded to your Gemini Enterprise app. To do this, you'll turn on document chunking, which indexes your data as chunks to improve relevance and decrease computational load for LLMs. You'll also turn on the layout parser, which detects document elements such as headings and lists, to improve how documents are chunked.

For information about chunking for RAG and how to return chunks in search requests, see Chunk documents for RAG.

Parse documents
You can control content parsing in the following ways:

Specify parser type. You can specify the type of parsing to apply depending on file type:

Digital parser. The digital parser is on by default for all file types unless a different parser type is specified. The digital parser processes ingested documents if no other default parser is specified for the data store or if the specified parser doesn't support the file type of an ingested document.
OCR parsing for PDFs. If you plan to upload scanned PDFs or PDFs with text inside images, you can turn on the OCR parser to improve PDF indexing. See the OCR parser for PDFs section of this document.
Layout parser. Turn on the layout parser for HTML, PDF, or DOCX files if you plan to use Gemini Enterprise for RAG. See Chunk documents for RAG for information about this parser and how to turn it on.
Bring your own parsed document. (Preview with allowlist) If you've already parsed your unstructured documents, you can import that pre-parsed content into Gemini Enterprise. See Bring your own parsed document.

Parser availability comparison
The following table lists the availability of each parser by document file types and shows which elements each parser can detect and parse.

File type	Digital parser	OCR parser	Layout parser
HTML	Detects paragraph elements	Not applicable	Detects paragraph, table, image, list, title, and heading elements
PDF	Detects paragraph (digital text) elements	Detects paragraph elements	Detects paragraph, table, title, image, and heading elements
DOCX (Preview)	Detects paragraph elements	Not applicable	Detects paragraph, table, image, list, title, heading elements
PPTX (Preview)	Detects paragraph elements	Not applicable	Detects paragraph, table, image, list, title, heading elements
TXT	Detects paragraph elements	Not applicable	Not applicable
XLSX (Preview)	Detects paragraph elements	Not applicable	Detects paragraph, table, title, heading elements
Digital parser
The digital parser extracts machine-readable text from documents. It detects text blocks, but not document elements such as tables, lists, and headings.

The digital parser is used as the default if you don't specify a different parser as the default during data store creation or if a specified parser doesn't support a file type that's being uploaded.

OCR parser for PDFs
Note: The OCR parser is in General Availability. To learn more about pricing, consult Document AI feature pricing.
If you have non-searchable PDFs (scanned PDFs or PDFs with text inside images, such as infographics) Google recommends turning on optical character recognition (OCR) processing during data store creation. This allows Gemini Enterprise to extract paragraph elements.

If you have searchable PDFs or other digital formats that are mostly composed of machine-readable text, you typically don't need to use the OCR parser. However, if you have PDFs that have both non-searchable text (such as scanned text or infographics) and machine-readable text, you can set the field useNativeText to true when specifying the OCR parser. In this case, machine-readable text is merged with OCR parsing outputs to improve text extraction quality.

OCR processing features are available for custom search apps with unstructured data stores.

The OCR processor can parse the first 500 pages of a PDF file. Pages beyond the 500 limit aren't processed.

Layout parser
Layout parsing lets Gemini Enterprise detect layouts for PDF and HTML. Support for DOCX files is in Preview. Gemini Enterprise can then identify content elements like text blocks, tables, lists, and structural elements such as titles and headings and use them to define the organization and hierarchy of a document.

You can either turn on layout parsing for all file types or specify which file types to turn it on for. The layout parser detects content elements like paragraphs, tables, lists, and structural elements like titles, headings, headers, footnotes.

The layout parser is available only when using document chunking for RAG. When document chunking is turned on, Gemini Enterprise breaks documents up into chunks at ingestion time and can return documents as chunks. Detecting document layout enables content-aware chunking and enhances search and answer generation related to document elements. For more information about chunking documents for RAG, see Chunk documents for RAG.

Image annotation (Preview)
If image annotation is enabled, when an image is detected in a source document, a description (annotation) of the image and the image itself are assigned to a chunk. The annotation determines if the chunk should be returned in a search result. If an answer is generated, the annotation can be a source for the answer.

The layout parser can detect the following image types: BMP, GIF, JPEG, PNG, and TIFF.

Table annotation (Preview)
If table annotation is enabled, when a table is detected in a source document, a description (annotation) of the table and the table itself are assigned to a chunk. The annotation determines if the chunk should be returned in a search result. If an answer is generated, the annotation can be a source for the answer.

Exclude HTML content
When using the layout parser for HTML documents, you can exclude specific parts of the HTML content from being processed. To improve data quality for search applications and RAG applications, you can exclude boilerplate or sections such as navigation menus, headers, footers, or sidebars.

The layoutParsingConfig provides the following fields for this purpose:

excludeHtmlElements: List of HTML tags to be excluded. Content within these tags is excluded.
excludeHtmlClasses: List of HTML class attributes to be excluded. HTML elements containing these class attributes, along with their content, are excluded.
excludeHtmlIds: List of HTML element ID attributes to be excluded. HTML elements with these ID attributes, along with their content, are excluded.
Specify a default parser
By including the documentProcessingConfig object when you create a data store, you can specify a default parser for that data store. If you don't include documentProcessingConfig.defaultParsingConfig, the digital parser is used. The digital parser is also used if the specified parser is not available for a file type.

REST
Console
To specify a default parser:

When creating a data store using the API, include documentProcessingConfig.defaultParsingConfig in the data store creation request. You can specify the OCR parser, the layout parser, or the digital parser:

To specify the OCR parser for PDFs:

"documentProcessingConfig": {
  "defaultParsingConfig": {
    "ocrParsingConfig": {
      "useNativeText": "NATIVE_TEXT_BOOLEAN"
    }
  }
}
NATIVE_TEXT_BOOLEAN is optional. Set only if you're ingesting PDFs. If set to true, this turns on machine-readable text processing for the OCR parser. The default is false.
To specify the layout parser:

"documentProcessingConfig": {
  "defaultParsingConfig": {
    "layoutParsingConfig": {}
  }
}
To specify the digital parser:

Note: Specifying the digital parser as defaultParsingConfig is typically not necessary. When no other parser is explicitly specified, the digital parser is used by default.
 "documentProcessingConfig": {
    "defaultParsingConfig": { "digitalParsingConfig": {} }
 }
Example
The following example specifies during data store creation that the OCR parser will be the default parser. Because the OCR parser only applies to PDF files, all PDF files that are ingested will be processed by the OCR parser, and any other file types will be processed by the digital parser.

curl -X POST \
-H "Authorization: Bearer $(gcloud auth print-access-token)" \
-H "Content-Type: application/json" \
-H "X-Goog-User-Project: exampleproject" \
"https://discoveryengine.googleapis.com/v1/projects/exampleproject/locations/global/collections/default_collection/dataStores?dataStoreId=datastore123" \
-d '{
  "displayName": "exampledatastore",
  "industryVertical": "GENERIC",
  "solutionTypes": ["SOLUTION_TYPE_SEARCH"],
  "contentConfig": "CONTENT_REQUIRED",
  "documentProcessingConfig": {
    "defaultParsingConfig": {
      "ocrParsingConfig": {
        "useNativeText": "false"
      }
    }
  }
}'
Specify parser overrides for file types
You can specify that a particular file type (PDF, HTML, or DOCX) should be parsed by a different parser than the default parser. To do so, include the documentProcessingConfig field in your data store creation request and specify the override parser. If you don't specify a default parser, then the digital parser is the default.

REST
Console
To specify a file-type-specific parser override:

When creating a data store using the API, include documentProcessingConfig.defaultParsingConfig in the data store creation request.

You can specify a parser for pdf, html, or docx:

"documentProcessingConfig": {
  "parsingConfigOverrides": {
    "FILE_TYPE": { PARSING_CONFIG },
  }
 }
Replace the following:

FILE_TYPE: Accepted values are pdf, html, and docx.
PARSING_CONFIG: Specify the configuration for the parser that you want to apply to the file type. You can specify the OCR parser, the layout parser, or the digital parser:

To specify the OCR parser for PDFs:

"ocrParsingConfig": {
  "useNativeText": "NATIVE_TEXT_BOOLEAN"
}
NATIVE_TEXT_BOOLEAN: Optional. Set only if you're ingesting PDFs. If set to true, this turns on machine-readable text processing for the OCR parser. The default is false.
To specify the layout parser:

"layoutParsingConfig": {}
To specify the digital parser:

"documentProcessingConfig": {
  "defaultParsingConfig": { "digitalParsingConfig": {} }
}
Example
The following example specifies during data store creation that PDF files should be processed by the OCR parser and that HTML files should be processed by the layout parser. In this case, any files other than PDF and HTML files would be processed by the digital parser.

curl -X POST \
-H "Authorization: Bearer $(gcloud auth print-access-token)" \
-H "Content-Type: application/json" \
-H "X-Goog-User-Project: exampleproject" \
"https://discoveryengine.googleapis.com/v1/projects/exampleproject/locations/global/collections/default_collection/dataStores?dataStoreId=datastore123" \
-d '{
  "displayName": "exampledatastore",
  "industryVertical": "GENERIC",
  "solutionTypes": ["SOLUTION_TYPE_SEARCH"],
  "contentConfig": "CONTENT_REQUIRED",
  "documentProcessingConfig": {
    "parsingConfigOverrides": {
      "pdf": {
        "ocrParsingConfig": {
            "useNativeText": "false"
          },
      },
      "html": {
         "layoutParsingConfig": {}
      }
    }
  }
}'
Edit document parsing for existing data stores
If you already have a data store, you can change the default parser and add file format exceptions. However, the updated parser settings only apply to new documents imported to the data store. Documents already in the data store are not re-parsed with the new settings.

To change document parsing settings for a data store, do the following:

In the Google Cloud console, go to the Gemini Enterprise page.

Gemini Enterprise

In the navigation menu, click Data Stores.

In the Name column, click the data store that you want to edit.

On the Processing config tab, edit the Document parsing settings.

The Document chunking settings can't be changed. If the data store doesn't have document chunking enabled, then you can't choose the layout parser.

Click Submit.

Configure layout parser to exclude HTML content
You can configure layout parser to exclude HTML content by specifying excludeHtmlElements, excludeHtmlClasses or excludeHtmlIds in documentProcessingConfig.defaultParsingConfig.layoutParsingConfig.

REST
To exclude certain HTML content from being processed by the layout parser, follow these steps:

When creating a search data store using the API, include documentProcessingConfig.defaultParsingConfig.layoutParsingConfig in the data store creation request.

To exclude specific HTML tag types, use:

"documentProcessingConfig": {
  "defaultParsingConfig": {
   "layoutParsingConfig": {
    "excludeHtmlElements": ["HTML_TAG_1","HTML_TAG_2","HTML_TAG_N"]
   }
  }
 }
Replace the HTML_TAG variables with tag names, for example, nav and footer.

To exclude specific HTML element class attributes, use:

"documentProcessingConfig": {
  "defaultParsingConfig": {
   "layoutParsingConfig": {
    "excludeHtmlClasses": ["HTML_CLASS_1","HTML_CLASS_2","HTML_CLASS_N"]
   }
  }
 }
Replace the HTML_CLASS variables with class attributes, for example, overlay and screenreader.

To exclude specific HTML element ID attributes, use:

"documentProcessingConfig": {
  "defaultParsingConfig": {
   "layoutParsingConfig": {
    "excludeHtmlIds": ["HTML_ID_1","HTML_ID_2","HTML_ID_N"]
   }
  }
 }
Replace the HTML_ID variables with ID attributes, for example, cookie-banner.

Example
This example specifies that when HTML files are processed by the layout parser, the following are skipped by the parser:

HTML element tags, header, footer, nav, and aside

HTML element class attributes of type overlays and screenreader

Any elements with the attribute ID of cookie-banner

curl -X POST \
-H "Authorization: Bearer $(gcloud auth print-access-token)" \
-H "Content-Type: application/json" \
-H "X-Goog-User-Project: exampleproject" \
"https://discoveryengine.googleapis.com/v1/projects/exampleproject/locations/global/collections/default_collection/dataStores?dataStoreId=datastore123&createAdvancedSiteSearch=true" \
-d '{
  "displayName": "exampledatastore",
  "industryVertical": "GENERIC",
  "contentConfig": "PUBLIC_WEBSITE",
  "documentProcessingConfig": {
    "chunkingConfig": {
      "layoutBasedChunkingConfig": {}
    },
    "defaultParsingConfig": {
      "layoutParsingConfig": {
       "excludeHtmlElements": ["header", "footer", "nav", "aside"],
       "excludeHtmlClasses": ["overlays", "screenreader"],
       "excludeHtmlIds": ["cookie-banner"]
      }
    }
  }
}'
Get parsed documents in JSON
You can get a parsed document in JSON format by calling the getProcessedDocument method and specifying PARSED_DOCUMENT as the processed document type. Getting parsed documents in JSON can be helpful if you need to upload the parsed document elsewhere or if you decide to re-import parsed documents to Gemini Enterprise using the bring your own parsed document feature.

REST
To get parsed documents in JSON, follow this step:

Call the getProcessedDocument method:

curl -X GET \
    -H "Authorization: Bearer $(gcloud auth print-access-token)" \
    "https://discoveryengine.googleapis.com/v1alpha/projects/PROJECT_ID/locations/global/collections/default_collection/dataStores/DATA_STORE_ID/branches/0/documents/DOCUMENT_ID:getProcessedDocument?processed_document_type=PARSED_DOCUMENT"
Replace the following:

PROJECT_ID: The ID of your project.
DATA_STORE_ID: The ID your data store.
DOCUMENT_ID: The ID of the document to get.
Bring your own parsed document
Note: This feature is in Preview with allowlist.
You can import pre-parsed, unstructured documents into Gemini Enterprise data stores. For example, instead of importing a raw PDF document, you can parse the PDF yourself and import the parsing result instead. This lets you import your documents in a structured way, ensuring that search and answer generation have information about the document's layout and elements.

A parsed, unstructured document is represented by JSON that describes the unstructured document using a sequence of text, table, and list blocks. You import JSON files with your parsed unstructured document data in the same way that you import other types of unstructured documents, such as PDFs. When this feature is turned on, whenever a JSON file is uploaded and identified by either an application/json MIME type or a .JSON extension, it is treated as a parsed document.

To turn on this feature and for information about how to use it, contact your Google account team.

Chunk documents for RAG
By default, Gemini Enterprise is optimized for document retrieval, where your search app returns a document such as a PDF or web page with each search result.

Document chunking features are available for custom search apps with unstructured data stores.

Gemini Enterprise can instead be optimized for RAG, where your search app is primarily used to augment LLM output with your custom data. When document chunking is turned on, Gemini Enterprise breaks up your documents into chunks. In search results, your search app can return relevant chunks of data instead of full documents. Using chunked data for RAG increases relevance for LLM answers and reduces computational load for LLMs.

To use Gemini Enterprise for RAG:

Turn on document chunking when you create your data store. Alternatively, upload your own chunks (Preview with allowlist) if you've already chunked your own documents.

Retrieve and view chunks in the following ways:

List chunks from a document
Get chunks in JSON from a processed document
Get specific chunks
Return chunks in search requests.

Limitations
The following limitations apply to chunking:

Document chunking can't be turned on or off after data store creation.
You can make search requests for documents instead of chunks from a data store with document chunking turned on. However, data stores with document chunking turned on aren't optimized for returning documents. Documents are returned by aggregating chunks into documents.
When document chunking is turned on, search summaries and search with follow-ups are supported in Public preview but not supported as GA.
Document chunking options
This section describes the options that you specify in order to turn on document chunking.

During data store creation, turn on the following options so that Gemini Enterprise can index your documents as chunks.

Layout-aware document chunking. To turn this option on, include the documentProcessingConfig field in your data store creation request and specify ChunkingConfig.LayoutBasedChunkingConfig.

When layout-aware document chunking is turned on, Gemini Enterprise detects a document's layout and take it into account during chunking. This improves semantic coherence and reduces noise in the content when it's used for retrieval and LLM generation. All text in a chunk will come from the same layout entity, such as headings, subheadings, and lists.

Layout parsing. To turn this option on, specify ParsingConfig.LayoutParsingConfig during data store creation.

The layout parser detect layouts for PDF, HTML, and DOCX files. It identifies elements like text blocks, tables, lists, titles, and headings, and uses them to define the organization and hierarchy of a document.

For more about layout parsing, see Layout parsing.

Turn on document chunking
You can turn on document chunking by including the documentProcessingConfig object in your data store creation request and turning on layout-aware document chunking and layout parsing.

REST
Console
To turn on document chunking:

When creating a search data store using the API, include the documentProcessingConfig.chunkingConfig object in the data store creation request.

 "documentProcessingConfig": {
   "chunkingConfig": {
       "layoutBasedChunkingConfig": {
           "chunkSize": CHUNK_SIZE_LIMIT,
           "includeAncestorHeadings": HEADINGS_BOOLEAN,
       }
   },
   "defaultParsingConfig": {
     "layoutParsingConfig": {}
   }
 }
Replace the following:

CHUNK_SIZE_LIMIT: Optional. The token size limit for each chunk. The default value is 500. Supported values are 100-500 (inclusive).
HEADINGS_BOOLEAN: Optional. Determines whether headings are included in each chunk. The default value is false. Appending title and headings at all levels to chunks from the middle of the document can help prevent context loss in chunk retrieval and ranking.
Bring your own chunks (Preview with allowlist)
Note: This feature is in Preview with allowlist.
If you've already chunked your own documents, you can upload those to Gemini Enterprise instead of turning on document chunking options.

Bringing your own chunks is a Preview with allowlist feature. To use this feature, contact your Google account team.

List a document's chunks
To list all chunks for a specific document, call the Chunks.list method.

REST
To list chunks for a document, follow this step:

Call the Chunks.list method:

curl -X GET \
    -H "Authorization: Bearer $(gcloud auth print-access-token)" \
    "https://discoveryengine.googleapis.com/v1alpha/projects/PROJECT_ID/locations/global/collections/default_collection/dataStores/DATA_STORE_ID/branches/0/documents/DOCUMENT_ID/chunks"
Replace the following:

PROJECT_ID: The ID of your project.
DATA_STORE_ID: The ID your data store.
DOCUMENT_ID: The ID of the document to list chunks from.
Get chunks in JSON from a processed document
You can get all the chunks from a specific document in JSON format by calling the getProcessedDocument method. Getting chunks in JSON can be helpful if you need to upload chunks elsewhere or if you decide to re-import chunks to Gemini Enterprise using the bring your own chunks feature.

REST
To get JSON chunks for a document, follow this step:

Call the getProcessedDocument method:

curl -X GET \
    -H "Authorization: Bearer $(gcloud auth print-access-token)" \
    "https://discoveryengine.googleapis.com/v1alpha/projects/PROJECT_ID/locations/global/collections/default_collection/dataStores/DATA_STORE_ID/branches/0/documents/DOCUMENT_ID:getProcessedDocument?processed_document_type=CHUNKED_DOCUMENT"
Replace the following:

PROJECT_ID: The ID of your project.
DATA_STORE_ID: The ID your data store.
DOCUMENT_ID: The ID of the document to get chunks from.
Get specific chunks
To get a specific chunk, call the Chunks.get method.

REST
To get a specific chunk, follow this step:

Call the Chunks.get method:

curl -X GET \
    -H "Authorization: Bearer $(gcloud auth print-access-token)" \
    "https://discoveryengine.googleapis.com/v1alpha/projects/PROJECT_ID/locations/global/collections/default_collection/dataStores/DATA_STORE_ID/branches/0/documents/DOCUMENT_ID/chunks/CHUNK_ID"
Replace the following:

PROJECT_ID: The ID of your project.
DATA_STORE_ID: The ID your data store.
DOCUMENT_ID: The ID of the document that the chunk is from.
CHUNK_ID: The ID of the chunk to return.
Return chunks in search requests
After you've confirmed that your data has been chunked correctly, your Gemini Enterprise can return chunked data in its search results.

The response returns a chunk that is relevant to the search query. In addition, you can choose to return adjacent chunks that appear before and after the relevant chunk in the source document. Adjacent chunks can add context and accuracy.

REST
To get chunked data:

When making a search request, specify ContentSearchSpec.SearchResultMode as chunks.

contentSearchSpec": {
  "searchResultMode": "RESULT_MODE",
  "chunkSpec": {
       "numPreviousChunks": NUMBER_OF_PREVIOUS_CHUNKS,
       "numNextChunks": NUMBER_OF_NEXT_CHUNKS
   }
}
RESULT_MODE: Determines whether search results are returned as full documents or in chunks. To get chunks, the data store must have document chunking turned on. Accepted values are documents and chunks. If document chunking is turned on for your data store, the default value is chunks.
NUMBER_OF_PREVIOUS_CHUNKS: The number of chunks to return that immediately preceded the relevant chunk. The maximum allowed value is 5.
NUMBER_OF_NEXT_CHUNKS: The number of chunks to return that immediately follow the relevant chunk. The maximum allowed value is 5.
Example
The following example of a search query request sets SearchResultMode to chunks, requests one previous chunk and one next chunk, and limits the number of results to a single relevant chunk using pageSize.

curl -X POST \
-H "Authorization: Bearer $(gcloud auth print-access-token)" \
-H "Content-Type: application/json" \
-H "X-Goog-User-Project: exampleproject" \
"https://discoveryengine.googleapis.com/v1/projects/exampleproject/locations/global/collections/default_collection/dataStores/datastore123/servingConfigs/default_search:search" \
-d '{
  "query": "animal",
  "pageSize": 1,
  "contentSearchSpec": {
    "searchResultMode": "CHUNKS",
    "chunkSpec": {
           "numPreviousChunks": 1,
           "numNextChunks": 1
       }
  }
}'
The following example shows the response that is returned for the example query. The response contains the relevant chunks, the previous and next chunks, the original document's metadata, and the span of document pages that each chunk was derived from.

Response
What's next
Create a first-party data store
Send feedback
Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-10-22 UTC.

Why Google
Choosing Google Cloud
Trust and security
Modern Infrastructure Cloud
Multicloud
Global infrastructure
Customers and case studies
Analyst reports
Whitepapers
Products and pricing
See all products
See all solutions
Google Cloud for Startups
Google Cloud Marketplace
Google Cloud pricing
Contact sales
Support
Community forums
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Google Cloud documentation
Code samples
Cloud Architecture Center
Training and Certification
Developer Center
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
Become a Partner
Google Cloud Affiliate Program
Press Corner
About Google
Privacy
Site terms
Google Cloud terms
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe

English
The new page has loaded.
